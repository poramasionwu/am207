{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project Testing (Autograd).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2qzYVWmQMjs"
      },
      "source": [
        "VAE TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3Ak66yXSNhx"
      },
      "source": [
        "\n",
        "from __future__ import absolute_import, division\n",
        "from __future__ import print_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQR89I1dUj7Z"
      },
      "source": [
        "MNIST Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTLrAfc2SZ0R"
      },
      "source": [
        "Source https://github.com/HIPS/autograd/blob/master/examples/data_mnist.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2yMsaSPSapi"
      },
      "source": [
        "#from __future__ import absolute_import\n",
        "#from __future__ import print_function\n",
        "from future.standard_library import install_aliases\n",
        "install_aliases()\n",
        "\n",
        "import os\n",
        "import gzip\n",
        "import struct\n",
        "import array\n",
        "import numpy as np\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "def download(url, filename):\n",
        "    if not os.path.exists('data'):\n",
        "        os.makedirs('data')\n",
        "    out_file = os.path.join('data', filename)\n",
        "    if not os.path.isfile(out_file):\n",
        "        urlretrieve(url, out_file)\n",
        "\n",
        "def mnist():\n",
        "    base_url = 'http://yann.lecun.com/exdb/mnist/'\n",
        "\n",
        "    def parse_labels(filename):\n",
        "        with gzip.open(filename, 'rb') as fh:\n",
        "            magic, num_data = struct.unpack(\">II\", fh.read(8))\n",
        "            return np.array(array.array(\"B\", fh.read()), dtype=np.uint8)\n",
        "\n",
        "    def parse_images(filename):\n",
        "        with gzip.open(filename, 'rb') as fh:\n",
        "            magic, num_data, rows, cols = struct.unpack(\">IIII\", fh.read(16))\n",
        "            return np.array(array.array(\"B\", fh.read()), dtype=np.uint8).reshape(num_data, rows, cols)\n",
        "\n",
        "    for filename in ['train-images-idx3-ubyte.gz',\n",
        "                     'train-labels-idx1-ubyte.gz',\n",
        "                     't10k-images-idx3-ubyte.gz',\n",
        "                     't10k-labels-idx1-ubyte.gz']:\n",
        "        download(base_url + filename, filename)\n",
        "\n",
        "    train_images = parse_images('data/train-images-idx3-ubyte.gz')\n",
        "    train_labels = parse_labels('data/train-labels-idx1-ubyte.gz')\n",
        "    test_images  = parse_images('data/t10k-images-idx3-ubyte.gz')\n",
        "    test_labels  = parse_labels('data/t10k-labels-idx1-ubyte.gz')\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbiIA4u2SODY"
      },
      "source": [
        "Source: https://github.com/HIPS/autograd/blob/master/examples/data.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zXhZ0r3SNSK"
      },
      "source": [
        "#from __future__ import absolute_import\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image\n",
        "\n",
        "import autograd.numpy as np\n",
        "import autograd.numpy.random as npr\n",
        "#import data_mnist\n",
        "\n",
        "def load_mnist():\n",
        "    partial_flatten = lambda x : np.reshape(x, (x.shape[0], np.prod(x.shape[1:])))\n",
        "    one_hot = lambda x, k: np.array(x[:,None] == np.arange(k)[None, :], dtype=int)\n",
        "    #train_images, train_labels, test_images, test_labels = data_mnist.mnist()\n",
        "    train_images, train_labels, test_images, test_labels = mnist()\n",
        "    train_images = partial_flatten(train_images) / 255.0\n",
        "    test_images  = partial_flatten(test_images)  / 255.0\n",
        "    train_labels = one_hot(train_labels, 10)\n",
        "    test_labels = one_hot(test_labels, 10)\n",
        "    N_data = train_images.shape[0]\n",
        "\n",
        "    return N_data, train_images, train_labels, test_images, test_labels\n",
        "\n",
        "\n",
        "def plot_images(images, ax, ims_per_row=5, padding=5, digit_dimensions=(28, 28),\n",
        "                cmap=matplotlib.cm.binary, vmin=None, vmax=None):\n",
        "    \"\"\"Images should be a (N_images x pixels) matrix.\"\"\"\n",
        "    N_images = images.shape[0]\n",
        "    N_rows = (N_images - 1) // ims_per_row + 1\n",
        "    pad_value = np.min(images.ravel())\n",
        "    concat_images = np.full(((digit_dimensions[0] + padding) * N_rows + padding,\n",
        "                             (digit_dimensions[1] + padding) * ims_per_row + padding), pad_value)\n",
        "    for i in range(N_images):\n",
        "        cur_image = np.reshape(images[i, :], digit_dimensions)\n",
        "        row_ix = i // ims_per_row\n",
        "        col_ix = i % ims_per_row\n",
        "        row_start = padding + (padding + digit_dimensions[0]) * row_ix\n",
        "        col_start = padding + (padding + digit_dimensions[1]) * col_ix\n",
        "        concat_images[row_start: row_start + digit_dimensions[0],\n",
        "                      col_start: col_start + digit_dimensions[1]] = cur_image\n",
        "    cax = ax.matshow(concat_images, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "    plt.xticks(np.array([]))\n",
        "    plt.yticks(np.array([]))\n",
        "    return cax\n",
        "\n",
        "def save_images(images, filename, **kwargs):\n",
        "    fig = plt.figure(1)\n",
        "    fig.clf()\n",
        "    ax = fig.add_subplot(111)\n",
        "    plot_images(images, ax, **kwargs)\n",
        "    fig.patch.set_visible(False)\n",
        "    ax.patch.set_visible(False)\n",
        "    plt.savefig(filename)\n",
        "\n",
        "\n",
        "def make_pinwheel(radial_std, tangential_std, num_classes, num_per_class, rate,\n",
        "                  rs=npr.RandomState(0)):\n",
        "    \"\"\"Based on code by Ryan P. Adams.\"\"\"\n",
        "    rads = np.linspace(0, 2*np.pi, num_classes, endpoint=False)\n",
        "\n",
        "    features = rs.randn(num_classes*num_per_class, 2) \\\n",
        "        * np.array([radial_std, tangential_std])\n",
        "    features[:, 0] += 1\n",
        "    labels = np.repeat(np.arange(num_classes), num_per_class)\n",
        "\n",
        "    angles = rads[labels] + rate * np.exp(features[:,0])\n",
        "    rotations = np.stack([np.cos(angles), -np.sin(angles), np.sin(angles), np.cos(angles)])\n",
        "    rotations = np.reshape(rotations.T, (-1, 2, 2))\n",
        "\n",
        "    return np.einsum('ti,tij->tj', features, rotations)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y--DVOCbaone"
      },
      "source": [
        "# VAE\n",
        "\n",
        "Define and test out a VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQuanHCxQWCG"
      },
      "source": [
        "Source https://github.com/HIPS/autograd/blob/master/examples/variational_autoencoder.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3E92gJ6QKjw",
        "outputId": "df425f6a-07ad-4a8c-81ef-b302f2b7d281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "source": [
        "# Implements auto-encoding variational Bayes.\n",
        "\n",
        "import autograd.numpy as np\n",
        "import autograd.numpy.random as npr\n",
        "import autograd.scipy.stats.norm as norm\n",
        "from autograd.scipy.special import expit as sigmoid\n",
        "\n",
        "from autograd import grad\n",
        "from autograd.misc.optimizers import adam\n",
        "#from data import load_mnist, save_images\n",
        "\n",
        "def diag_gaussian_log_density(x, mu, log_std):\n",
        "    return np.sum(norm.logpdf(x, mu, np.exp(log_std)), axis=-1) # https://stackoverflow.com/questions/43602270/what-is-probability-density-function-in-the-context-of-scipy-stats-norm\n",
        "\n",
        "def unpack_gaussian_params(params):\n",
        "    # Params of a diagonal Gaussian.\n",
        "    D = np.shape(params)[-1] // 2\n",
        "    mean, log_std = params[:, :D], params[:, D:]\n",
        "    return mean, log_std\n",
        "\n",
        "def sample_diag_gaussian(mean, log_std, rs):\n",
        "    return rs.randn(*mean.shape) * np.exp(log_std) + mean\n",
        "\n",
        "def bernoulli_log_density(targets, unnormalized_logprobs):\n",
        "    # unnormalized_logprobs are in R\n",
        "    # Targets must be -1 or 1\n",
        "    label_probabilities = -np.logaddexp(0, -unnormalized_logprobs*targets)  # Logarithm of exp(x1) + exp(x2), works out to the logistic formula\n",
        "    return np.sum(label_probabilities, axis=-1)   # Sum across pixels.\n",
        "\n",
        "def relu(x):    return np.maximum(0, x)\n",
        "\n",
        "def init_net_params(scale, layer_sizes, rs=npr.RandomState(0)):\n",
        "    \"\"\"Build a (weights, biases) tuples for all layers.\"\"\"\n",
        "    return [(scale * rs.randn(m, n),   # weight matrix\n",
        "             scale * rs.randn(n))      # bias vector\n",
        "            for m, n in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
        "\n",
        "def batch_normalize(activations):\n",
        "    mbmean = np.mean(activations, axis=0, keepdims=True)\n",
        "    return (activations - mbmean) / (np.std(activations, axis=0, keepdims=True) + 1)\n",
        "\n",
        "def neural_net_predict(params, inputs):\n",
        "    \"\"\"Params is a list of (weights, bias) tuples.\n",
        "       inputs is an (N x D) matrix.\n",
        "       Applies batch normalization to every layer but the last.\"\"\"\n",
        "    for W, b in params[:-1]:\n",
        "        outputs = batch_normalize(np.dot(inputs, W) + b)  # linear transformation\n",
        "        inputs = relu(outputs)                            # nonlinear transformation\n",
        "    outW, outb = params[-1]\n",
        "    outputs = np.dot(inputs, outW) + outb\n",
        "    return outputs\n",
        "\n",
        "def nn_predict_gaussian(params, inputs):\n",
        "    # Returns means and diagonal variances\n",
        "    return unpack_gaussian_params(neural_net_predict(params, inputs))\n",
        "\n",
        "def generate_from_prior(gen_params, num_samples, noise_dim, rs):\n",
        "    latents = rs.randn(num_samples, noise_dim)\n",
        "    return sigmoid(neural_net_predict(gen_params, latents))\n",
        "\n",
        "def p_images_given_latents(gen_params, images, latents):\n",
        "    preds = neural_net_predict(gen_params, latents)\n",
        "    return bernoulli_log_density(images, preds)\n",
        "\n",
        "def vae_lower_bound(gen_params, rec_params, data, rs):\n",
        "    # We use a simple Monte Carlo estimate of the KL\n",
        "    # divergence from the prior.\n",
        "    q_means, q_log_stds = nn_predict_gaussian(rec_params, data)\n",
        "    latents = sample_diag_gaussian(q_means, q_log_stds, rs)\n",
        "    q_latents = diag_gaussian_log_density(latents, q_means, q_log_stds)\n",
        "    p_latents = diag_gaussian_log_density(latents, 0, 0) # standard normal prior\n",
        "    likelihood = p_images_given_latents(gen_params, data, latents)\n",
        "    return np.mean(p_latents + likelihood - q_latents) #elbow\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "\n",
        "\n",
        "# Model hyper-parameters\n",
        "latent_dim = 10\n",
        "data_dim = 784  # How many pixels in each image (28x28).\n",
        "gen_layer_sizes = [latent_dim, 300, 200, data_dim] # latent, hidden_3, hidden_4, output\n",
        "rec_layer_sizes = [data_dim, 200, 300, latent_dim * 2] # input, hidden_1, hidden_2, latent (mu and sigma)\n",
        "\n",
        "# Training parameters\n",
        "param_scale = 0.01\n",
        "batch_size = 200\n",
        "#num_epochs = 15\n",
        "num_epochs = 1\n",
        "step_size = 0.001\n",
        "\n",
        "print(\"Loading training data...\")\n",
        "N, train_images, _, test_images, _ = load_mnist()\n",
        "# -1 or 1 ?\n",
        "def binarise(images):\n",
        "    on = images > 0.5\n",
        "    images = images * 0 - 1\n",
        "    images[on] = 1.0\n",
        "    return images\n",
        "\n",
        "print(\"Binarising training data...\")\n",
        "train_images = binarise(train_images)\n",
        "test_images = binarise(test_images)\n",
        "\n",
        "init_gen_params = init_net_params(param_scale, gen_layer_sizes)\n",
        "init_rec_params = init_net_params(param_scale, rec_layer_sizes)\n",
        "combined_init_params = (init_gen_params, init_rec_params)\n",
        "\n",
        "num_batches = int(np.ceil(len(train_images) / batch_size))\n",
        "def batch_indices(iter):\n",
        "    idx = iter % num_batches\n",
        "    return slice(idx * batch_size, (idx+1) * batch_size)\n",
        "\n",
        "# Define training objective\n",
        "seed = npr.RandomState(0)\n",
        "def objective(combined_params, iter):\n",
        "    data_idx = batch_indices(iter)\n",
        "    gen_params, rec_params = combined_params\n",
        "    return -vae_lower_bound(gen_params, rec_params, train_images[data_idx], seed) / data_dim\n",
        "\n",
        "# Get gradients of objective using autograd.\n",
        "objective_grad = grad(objective)\n",
        "\n",
        "print(\"     Epoch     |    Objective       |    Test ELBO  \")\n",
        "def print_perf(combined_params, iter, grad):\n",
        "    if iter % 10 == 0:\n",
        "        gen_params, rec_params = combined_params\n",
        "        bound = np.mean(objective(combined_params, iter))\n",
        "        message = \"{:15}|{:20}|\".format(iter//num_batches, bound)\n",
        "        if iter % 100 == 0:\n",
        "            test_bound = -vae_lower_bound(gen_params, rec_params, test_images, seed) / data_dim\n",
        "            message += \"{:20}\".format(test_bound)\n",
        "        print(message)\n",
        "\n",
        "        fake_data = generate_from_prior(gen_params, 20, latent_dim, seed)\n",
        "        save_images(fake_data, 'vae_samples.png', vmin=0, vmax=1)\n",
        "\n",
        "# The optimizers provided can optimize lists, tuples, or dicts of parameters.\n",
        "optimized_params = adam(objective_grad, combined_init_params, step_size=step_size,\n",
        "                        num_iters=num_epochs * num_batches, callback=print_perf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading training data...\n",
            "Binarising training data...\n",
            "     Epoch     |    Objective       |    Test ELBO  \n",
            "              0|  0.6932584206858606|  0.6932658793488439\n",
            "              0|  0.6842849129464189|\n",
            "              0|  0.6452992015955531|\n",
            "              0|  0.5321971419593212|\n",
            "              0| 0.40756819418767254|\n",
            "              0|  0.3228370999978949|\n",
            "              0|  0.2811445004050572|\n",
            "              0|  0.2974902286890794|\n",
            "              0|  0.2605104243037134|\n",
            "              0|   0.247576301366411|\n",
            "              0|  0.2705762342910167|  0.2544951504447709\n",
            "              0|   0.273363533743689|\n",
            "              0| 0.23792007931717812|\n",
            "              0| 0.24117258074155562|\n",
            "              0| 0.24310971905297266|\n",
            "              0| 0.25596724480141814|\n",
            "              0| 0.24410074875637935|\n",
            "              0| 0.23731461494549388|\n",
            "              0|  0.2419736876383133|\n",
            "              0| 0.23539889313327136|\n",
            "              0| 0.23150711444336194| 0.23249634347554238\n",
            "              0|  0.2264712342246887|\n",
            "              0| 0.23809118033676369|\n",
            "              0| 0.21985719443066054|\n",
            "              0| 0.23506102619534716|\n",
            "              0| 0.21815026739815196|\n",
            "              0| 0.23453779432658475|\n",
            "              0| 0.23073486602957552|\n",
            "              0| 0.22514159310916884|\n",
            "              0|   0.221719539658667|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR8AAADuCAYAAAAA7gNFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO19WZMbyXX1xb43gF7ZZJPUzGiksUd2hOzws/33HWG/OMKSJcdQ5HBp9o59X76H+s7tk4kE0ACqUCSd5wXV1UBVZmVW5rl7Yj6fi4eHh8e+kYy7AR4eHv834RcfDw+PWOAXHw8Pj1jgFx8PD49Y4BcfDw+PWOAXHw8Pj1jgFx8PD49Y4BcfDw+PWOAXHw8Pj1iQ3uTLx8fH89/85jcRNcXDw+NbxH/+53/ezufzE/v8RovPb37zG/mP//iP8Frl4eHxzSORSLxznfdil4eHRyzwi4+Hh0cs8IuPh4dHLPCLj4eHRyzwi4+Hh0cs2Mja9X8N3W534VwikRARkfl8rsf42/U9G/ie/X/XeVeit0QisfS8iEixWHTeF5hMJiv/vwzc73XfCzNBXTq9eopu2584sa5P0+l0Ty0JB6lUaqvfeebj4eERCzzzWQF7t99kV+fv2SxJRGQ2mxn3cTGnZSzLxaqiToe7Tb+3xVNZlsfXDb/4PAHLXnzXy4Fzs9lM6fN0OtXFhs/xNXFdUNh0Oi3JZFLP4di18CwT8db1Z5uX+yli166IatFZJdrO53Nj7DBe+JzP5zoGIqLH/InrhvEMogBvYsue8T7b7sUuDw+PWPDVMh9euaNarVcpdu1dBN+FAnQ4HMpwOBSRYPfEed5pmc1ACcnXwf8zmYxkMhkRCViQS1m9yTPYhfGs2zHD3FGjGlceD3yOx2MZj8d6PBqNRMRkqGClmUxGcrmcHouI5HI5HUNmQfuGPU7z+dxgb/b/WWGfyWSMPohEy4S+uMXHfiidTkeurq5ERKTVai1Q25OTE6nVaiIiUigUdDKE8dDW6VaYmmOy4nM4HOpknkwmCzQ+mUwaIha+i8k+m830/+PxWCdFLpeTbDar1wirr8uwajKz3orFDxZPAP4Niz+2+OK6dxjtn81muhn0+321ZDYaDREReXh4kH6/LyLB2PV6PRF5nIeZTEby+byIiJRKJSmXyyIiOvcODg70XD6f13no6leYsOcj5g/6cn9/L81mU0RE7u7u9DwvvqVSSftychLEfx4eHoqISLlc1rnH6oEw4MUuDw+PWPDFMR/skNh5Pnz4IO/eBUGxnz9/lsFgICIip6enIhKwAuwy6XRaV2Y+jgLz+Vx3j9FopLsq2t1qtbSt/X5/gebzLuJiC+l0WhlOoVCQQqEgIsFOhB0W1D/s3dWleAUzm81mhnhiK8pt5ThfC3CxPxZbwho3HqNer6cMoNFoyP39vYiIXF9fi4jIzc2Nnru/v1dmhPbncjmpVCoiIlKtVuXFixciInJ+fq7PAvcqlUrKJnK5XGTjg2MeIzBv9PXz589yeXmp/Wq32yIiOjczmYz6hr169WqBVaMP+K4tlu0Cz3w8PDxiwRfFfFgRiJW52WzKx48fRUTk48eP+n/I36PRyNiJsTLPZrOddSKrvJZZh9Dr9aTT6YjIo1f0zc2N6hPa7bbuSPx7gBXKuGc2m9Xds1gsqm4B/8PvRMJRcLpcBMbjsY4D+jUYDPSY9VroDzOfbDarxxiXfD5v6K+wq4rIAovatU88Rp1OR8ej0WjI7e2tiIiyndvbW7m7uxORYDzRbzyXXq+n/R6NRspE8VksFnVcmLVmMplImA/r0JiBg3mD4XQ6HW13p9PReYrvpdNpnUc83hjX4XBotJ91rruOT+yLj20pgkIMtJHp8q+//qqT9ejoSETMh2/7Z9gPZ1t/GFdbWSnJExvK8evra53Yg8HAWGxETJGD6SwrobFg8aJaLBYXroW2bYv5fK6TjZWxrVZLWq2WiIi+rA8PD/Lw8KD9ssMbuC8ij5T94OBARAJFJsauXq8bYjIWvW3c9VnEW+drlUwmFxbwYrGoL9lwONTz+H273dbjXC5nWJBEgoWWrxmFyM9zG22ZTCbGQoGxw+dkMtHxyGazC+I/Lz4ij3Me95pMJnqvRCJhzM9d4cUuDw+PWBA78xF5XGWn06musuzd+/nzZxEJ6DJMgBBJyuWy07wexc4zn8+NHYcpPZgB2npzc6MUN5lMahtB09PptNMvhJWHLMpgJ8MuF1Z/0Bc2zYKx3d3dyc3NjYiIvH//XvsFNjSZTHS3B8PJ5/Pal+FwqMfHx8fafvyGFbNhiY0M3qlzuZyKrqlUaoG5ZDIZZZq5XE7FejCE29tbZeDz+Xyh3+wCweyPGVlY4r/dR547zCRFRI0TIgGTtZlLoVDQMWAXAWbi24QXPQVfxOLjWjTQSRYDRqORTgpMpHw+b7zQYfq+uB4003hM1na7vaBDaLVa2pZ8Pq9ih8sXhP2EIHOPRiM91+/31SIxmUwWJrPt+r9p/8bjsSFCQl9wd3enlhIsPre3t/q7g4MDQ4QSCSYzXtKrqytdgNHXWq3mpPw8druCrYgsVrleKDxjXkgPDg70GOPRbrcNUQQvMet8MEas6wrbNwbXZNEUczKdTut5vCfD4dAYZ2yY+A2Li5VKxeiDyKKD67YR7C54scvDwyMWfBHMByvzdDo1lGcigYITbKLb7SqDODs7E5HA5wKrfDab1ZU5TBd3WyRCm9HGRqOhCmfs+tPpVFlOtVpdYAis4BwMBgv+F5PJRO9rB6GiHWEFMi6LuJ9MJspc0K5cLqei76tXr+TVq1ci8shEZ7OZsqSHhwcdT7DXVCqlu2o+n9exy2QyoVm5uF8s/vD1bT+eXq9nKJzRbsy9m5sbFU1zuZzOQ4znwcGBwcCjwDK1gkvEY9aMOfnw8KAWPfgssSW1Wq3qnGVRjNUDYY7RF7f4YJIDs9lMRRq2+uCBFYtFlbvXRX9vCldKDVyfF59ut6svKQY9m81quw4PD9VtncVFDqmwo92n06kz+pqtNmEtOmwaZvGBLVD4f71el++++05ERH788Ud1tkNfu92uWvzYIohJnclkjAnOlD6KUASeDzhmvRlvcph7hUJBv8Pmeba0YtHBQsym9jD0POv6hOunUinD4miHV7BrxNXVlR5jcalWq9qHw8NDHXt2LFwVArNTP0K9moeHh8cTETvz4Z2BV3DQu+vra2U+mUxGrSbYeTjwLeogPlYeJhIJQ0mMXZNFJOzw5XJ5QZEn8rg7tdttPWb/DHYixPNg0TSM/uD62OmKxaKKQhwtj7YcHh6qyHtycmKwTrSfAxnBLND/arVqsL+wRa1lfbQtNZhrsNxdX1/rMVs1wdym06nOvXK5rGI0xphFflwjrH65roG5bvuzscOhSMDsYLH8/PmztgvtPj09lWfPnolIIDra4xllhL5nPh4eHrEgdubDruKpVMpw9RYR+fOf/6xyd61Wk4uLCxF59Jhls2YUbbPBOw6HHmCH5EBEZmTYkaBYHo1GynKazabuumAKvIvyMfsXhbW7MrPiZ8n9BxsqFAr6nWazuZB64vLyUt6+fSsigZKWzdciAXPCrutiClGCdXgYO7R/OBzqPBsMBnqe3SVsHxr+/zJsmmvpKXB53osspuYVCfqHwOyrqytVNEPPc3Z2plKEi4l+8/l8mP7jGJPjzZs3epxOp+X58+ci8kjjoxS1ViXI4ujufr+vNN2OOBYJfGdssYxFNV6IXApSO37Hzv3DCshtwU6dAG8GHMf26dMnEQksQbDwoE0fPnyQX375RUQCJe3r169F5NG6UqlU9B72xhO2knaZj40dkoPvoq/dbtdYdESCxRP/Zysgxs3OexM32IqH2MhOpyPValVEHrNC1Go1ZyT7PuDFLg8Pj1jwRTAfjoKGogxiCOivSOBXgh00TE/LdXAxIBZ/RMwcNWgf2ECr1VqI1udoYQ4U5Lw47F/EOzW+46LZu/ZRxAyVwE7J1B5jw3mMMG5sIEgkEqpchlKzWCwa12LXhbBhX9N1D7CZSqWiSn8W5SGSJJNJZTasVAcDqtfrS0MSwoadwpezAbBBQyTwSGdRHmoLFn05INf2aYtS4fxFLT4sPiDJ03Q61UF98eKFiluuh78P8IvDicFAzznvMqf6sEUl9mtxpSZlPVGhUDBkcNuXKcxyNZlMRkWpZ8+eaXuwiPBmwLmMsSC1221tN6flBM2vVCrGs4rq5eRPHLPoisWSMwuwNQvPAG3t9XqqExoOhwupKUajkT6LXbMpPAWuvMx8H06tgQWpWq3q4gM/n8lkos8inU6H5kP2FHixy8PDIxZ8EcwHmE6numJjl0mlUob7d5Q0/SlgT2O25HCwpMii5cAWE1msajab+nsOEuQEY/h9sVg08q/g+ruA2VQ2mzWiv7FDsscsnkEymTQCL9F+MICXL1+qwhnMh9vPzyCKnZZ9rqbTqfaBw2HQbpHHyPujoyO1BoEVXF5e6tiORqOF9LLD4VD7zYG+UagHOG/VZDIxKnAg1xJ7mUNaOD8/V/8kYDQaGcYS13hEVSnGMx8PD49YsHfm41ICcq0rOy9OIpHQXfPs7Gxt/agosKxcDnaUer2+kC40kUgYeh7bnJlMJo3aUHZGQDsDHXQPuVxO9RHb7ELLcrKwDwvrraCYZMUy2trv91U3xzF58IB+8eKFBp7CUMCMkO8b5o7K8XGs6Ide6ubmRr1+cY4DZk9PT/UZc6Aw67s4rxOuzylh7cyUYYLTw47HYyMXExgPGNBsNlOjwYsXL/QY7R+NRsY7ter9CjtmLTaxixOI8WSGTwKiiUUelWOYEPuCK28O2p1Op3XxGY1GOsk4TaWdsIrBfkL8XRZ/uHoF5wPaJXnauqJ+7OvBkeZ4sSaTifq2cK4ljnqH2GY7sNnXt8NVwkooxiIJVxPBnPr48aO+nHiJs9msYelBfyCecU7uSqWi32WRB8ec9ybMTZL7x6E9aNf9/b1GrWNRTafTuvAfHBwstNuVkteGD6/w8PD4phCb2OXanTi4D6zg8PBQfUSOj48XKkFGaRJcdW1bJOEk2yKm8s5Vcpd3nHQ6rcyAFda4fqVSUbpcqVQi8aR1MRDuAyv6OVQE4wXqX61Wta31en2B8WzC1rbtHxsFwGza7baK8tfX1youYhwymYwyCBHTY1skYD7odzabdVYkdTHdqNJrcH07KM2vr6+V+XDKDcwjlhyYYa9z2eD2h9mHvS8+duNtCxeLWyLBBGYHNdvSsw/woHCaSk5tyrE09u/YfwLgfD3T6XThJS2Xy0qXj46ODJ1JlPW0bVGINwmRgObDx6XVahn9FQmoPUetuxYv+37LsK3Iwtfk/DZo9/39vRHHhb5Av5PP5/U8fjOdTo3QBE4iJmLm5LZzOEcR28X5tzEGvV5voWw3O0fy82SdlKt9UTpJAl7s8vDwiAWx+/mw5p4tJhxFDd+ESqXiVLZGBdfOy8yHlXa2X0c6nVYlre1vImIm82bLGAcysn8TmE82m91rACMHwooETAAMgZkPK8fRB95VWREflbXLpTRn6yKHIYDRcY0ysKF8Pq9GDsy34+NjDWp++fKlZnDEGLFXdNhlhVdlV0gmk9oXtkTyb13PY1n6WvsZbmPUeCo88/Hw8IgFsTEfVzBcsVhUXwuWOeHnc3R0pGxiH+H/tiKOd4ZMJqO7I8v72EWy2azB6NivAtd0+dZAj3RwcKDPgpPkLyu/G4VJl8sDcWkdjuPidCciZglkZjisjI06VwyzU7S/Vqup/xHr6MAox+OxnmN3Ac57A7Zzenqq+h+MF5vX2WM8DLgUwuyLxfmY2eUBv8XcqVQq2i/0u1QqGWWe7XZ/U/l8bFqXz+fVrT2Xy8n5+bmImEm5WBThJN1Rw6V04/az46BdL6lUKuniw8F77FgIsPIZk6ZcLqsykydIGCEJqxSMfMy1u7H4NJtNI9UrtwvtZuuKLX5EHVLB181kMobyG8fn5+ca9sHJ3XhDxOKCz+PjY+1XsVhcmf416rnJaW8PDg6MAoFYIDmSnQscsigvEowRW+72GaTtxS4PD49YEJvYxQotKOpyuZyyINd3951pzd4FbNrLOzj75+C7dp0vEdMtn4NJbT+hQqGgu5tdAdPGtiLXOm9nBtN8jFelUllQzJZKJaX05XJZ/49Pu7aa675hiZDJZNIIewFzOTs7k59++sm459eAZawb41Gv1xfmER+zaMgi8b7fKyB2a5ftV7LsO18CnhLpy2717F+B7+MlnU6nzty7LJ64RJVl+Ys2eUausBG+Du6by+WcYR94iY+OjhbEaM7Xw75QrAdyvRj8jKKA/Xw2nVNxZVFwgceIfcjm88ca8q4EYfw71zu37/fMi10eHh6xIHbmw/hSGM4qLIv2Xdd2+3fLRA7XjrQsW926dq3Dst+4gluh/LYV06si5J9i9Yk7L9PXCpcoJfL0fn0J75pnPh4eHrHgi2I+Xxqgr/iWwDWnvgV8a/0R2W9xhDjhmY+Hh0cs8IuPh4dHLPCLj4eHRyzwi4+Hh0cs8IuPh4dHLPCLj4eHRyz49uyUEWAfKSX3hV6vt1EqUxf2+TzWuTsgS8DXBLueug3OePA1YFvXAM98PDw8YoFnPk/Askjyr5UJbZIwfFOWs8tzWRXwGhVcAcLrov237WMchS6/5DnqmY+Hh0cs+OKZj6tUMeMpKTmiwFPuxbvPNrvel9Afmw24Mh7a33d95ympG3ZNe7EOrnZxWpNVc82eZ65A4HV9jHo8XYG+2+r39jH3vojFhycCl1EWCVJ5IoXncDjU/3O1BOSKsTPxxwHXy7auGJvrO6vav8++rXpJ1y2orpzDdhI2xr76NZvNjGoa9jlXmzi/kqsPnEqXF+B9pvtdl2WAv7tJFoao+uDFLg8Pj1gQG/PhSphIdt1qtbTcK8rZXl5easnah4cHTcf58uVLERF5/fq11lM6PT1V0+y61KO7Yl1GQZvC2wnkeZdNp9NGJQuRxTpT3JddTeXL+rPqHJgo94XLRANcIRNtt6/FKVld9a3CHC9mbHjm4/F4YTxGo5GzoicyT2azWSdD5za7+hV2f+x+8TEzn3WZDJdd0y75zOJk2Anm9774cPlakaAUy8PDg4iIvH//Xv7yl7+IiMif//xnERH505/+JFdXVyISTCCk8Pzhhx9EROTnn3+WP/7xj3p9lEdJJBKR1nVfp8fhicCiI6oljMdjHehUKmUUnMMnFwfkvtg5rcOwvOH3LFbxC8tVN9CXbrerxQ4Be9G0xRNeaHO5nPaxWCwulEXatU/z+dwoWYRKHN1uVwsEYh62223dBLkN2Mzy+bzRPrtUUiaT0fSx2WzWSB8bhe6KFxfuo13emgtacmkcbhN+Y4+93VcerzD65MUuDw+PWLBX5jObzXSnxI7T6/VUxLq7u5Nff/1VREQ+fvwoIsGOhJWdS8Ni57q8vFQR7Pz8XOsR5XK5SKteLEsnylUqsPv0ej0VJz99+iQigQiJZ8BVO7ioG0TMXC5nJKC3lYZh7EK8Y7J4AjbQ6XREJBC1UDSw0+loDS8uhsgJ6LlmlEjAJLhIJNgC94ML3m0D3r3R/m63K/f39yIS1B77/PmziIiy7kaj4VQ4o635fF6rcuRyOWU2XKOMj7lsdljjxHPLxeja7bbOKYwLFypIJBIL3sjJZFL/z5UwwLTz+byOEYtgdiGAbeCZj4eHRyzYu87HlUgdO0Y6nTYqRIqY5V4LhYIyJ6zQJycnumuPx2NDgRl2Okqb7bBbALsGiAQModlsiojI1dWVvH37VkQeGd3V1ZWyhVKpJEdHRyLyqEg/OzvTMtG1Wk132kTisUYY64x26c98PjfKIjMrtXVVjUZDWWe329Xvot+z2czQg4C9YYwPDg50jO2dFCxjXezTU/s1Go2UAdzf38vt7a2IBM8eekQwH7tiKXZ+sKHBYKAMo1wu63nWp+AZZLPZBcVtGP3B9Xu9nvar2Wwqo/v8+bPB7tB+LsHELAb9xv/L5bLOw5OTExEJykVzQQE8lzDerb0vPqxgFAkGDw/18PBQxQ4ojn/44QctoVwoFIyyvSLBi891zF3WoqisJ7zg4OXFBG00GrrQvHnzRt69eycioi/Azc2Ntqter+t1ma7jGdniid0WPt6mfpddFhkLTbfbVXELYuP9/b2KXd1ud0HBmUqldAwGg4GxKKFfXFLarvu1rH+b9MdlSeWFlBdN/KZQKOiGxmWeeYNh3yAu/mi3la2Tu2I+nxviu0gwFhiPy8tLFeXfvXun8wvjKWLWZYdIi351Oh29frVa1XtgHhaLRa1cErbi3ItdHh4esWCvzIcVXlzuFbtTMplUBgGa/v3338uzZ8/0N6CVWKF5hc9kMgYtDGulXuZTwdSaFYAigc/S+/fvRSSgw2gndv3Dw0OjJhZ2JGYKti8Q+uVSpG/DePA5mUz0vmyuZbEFDKjVailrSCQSys64ffwscF0uDQ1kMhnDO9024247fjwuy3yVcC/Ms2KxqAyhUqksiF2tVkvFzel0uuDnwyWMV5WE3hTz+dwQt0QC1n9zcyMigQEDrPrNmzc6Trj/6empqjBOT091/oEhtdttlSLQdxHRd5KZbJiMTiSGxQdgmRHn7+/v1QpRr9f1f3jot7e3SjdxLplMOidNmA9pWbQzLwK4L77b7XZ1wSkUCkpd8Zter6ffzefzhtiCT/YxwfXZV4P7uInY5eqPy6GRHeXwAvCCwxYgTOrZbKYvwMPDgy7GeFnZUlQoFAx/GPsZbgqXFZD1Hbw4QMRiaxzGqFqtalswz1qtlvZlPB4v/L5QKKh+K5/Phxbm44o3m0wmhn8S3ol+v69twIb929/+Vn3iDg8P9RrQQd7d3enYdjqdhbHP5XI6tsv8hLaFF7s8PDxiwd4Vzq5gQjCE6+truby8FJFHBWe329XfjMdjpc7YmU5OTpQ6VyoVg7pH4dbuEnVc3qLD4VDPHx4e6k4E3N/fGz5P2FVx/VKppLsY+8aEIU6yX4dIsKPhueXzecNaZItorDA+OjpSyxWuNRgMlPmwOIfxYn+YSqWiDCKdTu9sQXFFmuOa7HPEzAffZZGdrW2Yb71eT/vFbIT9gFwBzruGJLB1E9dnJjydTvUZl8tltZD+/PPPIiLy008/ycXFhf4OTA7WPmZRPAaYe6VSyRC7mDV7Px8PD4+vErEFlmLVnE6nynL++te/6oqM/19eXuquyko++CHwLmAHYO6KVXoUO6eL7R07Go10F6nVaqr0g5l6NpsZvhjYVbFzHR0dqdvBOh3Ctn11+bWgbTjvOgfmUy6XjfgykYCpQknd7Xb1utAN1Wo17VexWDR0dLsqml1pLrBrp9Np4162H85gMFAW02g0tF/QQV5fX6sLAs85l+d2mMYOm72JBOwR92fftlQqJS9evBARUbZzcnKiv0ulUiplYB7e3t7qOZYiDg8P9V6uktRh9C/2fD6z2Uw1791uVx80K/dAC1OplE4a9tN49eqViIj8+OOPoaaqdImI6xKEcUAfXrhyuax0F5P53bt3+pImk0ml91h8OEKfXfRd2JQC299lh0x2SkM/+DcsUnS7XW03+tJut42gTSw0MCBUKhVDgemi8bvmwnH9jgNyU6mU3gPjMp/PdQNIJpP6QmKDYOV5Nps1Fm588kLsiirfti8uXyiIWhzYWywWdfFAuzqdjqH0RyjTmzdvRCQQv3CNYrGoToYQpzmcJ+w0sF7s8vDwiAWxMR8OksPuUywWVZwCVWQ/nlQqpTsRPq+vr+W//uu/RETkD3/4g4o3YYdWuMA7tb0rsOJ2MBhoH7Hz3N/fG96kYAZgcSyS4B58T1c7nopVqU1tUcsWZSaTiSFWYVcFa+h0OnqcSCR0B3WlmGAvcTvAkdv51D65FM4shjNzYM9nfLKPFYeYiJjhF3gOIiYDYdGbFfS7wmY+/KxscRKiIQK0r6+vVXk8nU7V6/5//ud/RCQQu/DOlEoldTfANZeN0Tchds3nc7U8vHz5UmVVDsNgmg66iIf36dMnpfmfP3+Wv//7v9fvRmHtsttuA209OTkx0r/yIioSTFaOA0O7MREKhUJk7XeJk7wg8MvLoRAi5ks6mUwWYr/6/b6eOz4+Nlz7RYJx4dAEl6iyrJ1P7ZNrXFgPwwssx7RhjKbT6cJ4zWYz1emwUymeBeuMWCcUhRogmUzqYl4qlXSB540c/crlcvrs5/O5Lj6wKk8mE/3/+fm5Lj7c77DFLcCLXR4eHrEg9jSqo9FIWc75+bnhQyISWEdYwYkdBdHI7Enc6/WcgX5RMwjOYYMd6fT0VHehfr+v7Iw9nVncBONz+b1E5bMEsMjCyth8Pq+7Ivvr8PO2/a5arZZeq1wuGxkJ8D0OrWGEpXBe1kdWDrMVDPdi/x6MI/o3HA6NwFU7oNYOPGVxK6z+4Hnl83m1Rn333XfaVk5xC+RyOSNkAuoKiJ2lUklF/tPTUzUQYLzCTp1q9CeSq3p4eHiswV6Yjyswk+NJsJNmMhld0eERbKddwE6K1Zrzx4iYwYPLdtZdsEzJzLuSiOkPMxgMjOx+IgEbQr/Pzs40bUhUcTTr+sR6Ho53wk6Ic61WS9tdKpU00BdKZm53rVYzsvvhmi6PWZfye1fwGLHCmb2swT4ZHOQKJnFzc+NMGwKMRiPDFB+WqZ3B7Br+PIVCQdPPjEYjIw+1SPC8wdLev3+v7cJ4npycyOvXr0VE5MWLF8qS8P8oy1HtZfFhhy4sOlBQ3t/fG5G4nFNFJOg8WyYgbv3tb38TkeDFxkJ0eHho+NlEkc9nGeyAPJ7sPAE5BSlwdnamLwMGPWpRC+0CeIHnl9RW0iaTSSOq3a4+kUqltC/lclmPseiGEUaxrj+u4pLJZFLbUC6XF9KNZrPZhVxTImb2BPymWCwuOL6yAWE6nUYy93AtXsALhYIaKYbD4UIg73g8VlGLw5NgVX7+/LlaWKvVqvadg3xd9crCgBe7PDw8YkHkzIepL+eHQUjFu3fvVBlbKBSUEeBcOp1Wav/LL7/Iv//7v4vIo4fmdDpVZS2LaCx2RaHAXAaXOXc+nytjQ9a5Xq+n7O7s7EyP91lx1ZVag89zGgreyeAciwIAACAASURBVNkfxlZw5vN5Nf2WSiVDgY5rLmtL2Apnu1+u9K4QKweDgZFrCnOHFe2cFoTDg/A9fkZ8HFZ/eFxcY1QsFpX5g6U1m009Hg6HOg6cLhXMqVgsLoyP3ZcwsRexCwM0GAxU7EACpF9++UWPy+WyUkTInr1eTx2m/vSnP6mTHq75u9/9Tn766Sf9jWtyR23tcpUQ5soJnU5H/SqwCCWTSZXVj46OQq9ZtSlcE4wrH7gSj/X7fZ3s+F2lUjF8e1x1nvhljKKfLl0cJ39jcZFFLIB9kSB+FItFPZdIJJxhJ1xoMIqKKSwa87jwvewy0JwWt9PpLIj3Jycnhg+WPf+8tcvDw+Obw16YD9NpTuItEoghEKHAikQeKXKv11MRbDweK6X/8ccfRUTk3/7t3zR3yenpaWQR7qvA12dFH3acy8tLZT5gQ6worFarW1WCDFMsce10iURiIT1sv9/XsWPfF1aA2rurfc9l4skyMXBTsEjChgBmxVzaWCRgNq6Uq2zAYKW77SfECehZeR0mu+PruBgdJ8znQF+8c7PZTN8fWFWLxaIhEtuMbtkYhYHIFx/WlrOszakIMND39/f60LiEK6jxycmJ/OM//qOIiPzrv/6riIj80z/9k9Zqz+fzhvUlCuq7DlwOGovm7e2tLqxoU71eV4sDF5kDVplod5kArhioVWENduUEDkPgiGpOdMUipG0VYkvUMn3CtouQHQOVSCScjoWu9B38ErPI7CqGmMlkdKFhyx5baMPc+Oxr2JsGJ7DjcRIJRC3ofLLZrC46XNoZz4hLQEUtGot4scvDwyMm7EXswsrKosbvf/97EQl2Cewe//3f/61Z+bHz5HI5ZTb//M//LP/yL/8iIqJJscvlcuSpU58CVsiKmGk3h8OhPgMoYw8ODtSh0u5DlFh2fRfL4PLWGI/hcGiIIvgdpyDFsYsB8BitU2Zu+yxY7MJzzWazhuWK66OJmKWhu92ufpf7B2ZzfHysxgLkX6pWq5E7iD5ljMBKMfdY7OJUsczSXI6z+1BZeObj4eERC/aqcGZzJHQ/z58/lz/84Q8iEpihoSdhnwqwpaOjo4VMbXExHRvLdiWRoK3Q72CnrNfr8v3334tIoMuy/Wn2iWWhDa50CpPJRL/DilXsqOVy2fDzYc9mkUVDQJSewLavFet0bN8bZgDMkljJDGbDAZhcOsilUwoTLncFTiDPfnTsf4Ux4GNWOGPu2bmBcM+v2s+HYU8MrtKAeJWvAbbzmD1A2WxWJ+bZ2dlCkiZ27iqXy07nrmX3jQIsCvGCgomJRV/kcWJzgUHOLwwDQTab1fOuWKFlL2mYTnmc/xjPvlQqOVODrotaB3gh4npq+9w4XPdKp9P6vPFOcb6e2WzmXHywkNZqtQWnUK9w9vDw+OYQeybDbwHsMctmZhzXajWlxuwXgh2JFbMucXLfOyp7/zKTEwl2VJhuOYCSg1FZ+Wwzn2WR7HYbwuwPt8++vsuszyIaZy/ka9lVY/dp7GD3FY6gZwU75la1WjX6Zpe3tqvGugwEUcEvPjvA5fSFly2Xy6mocnR05NQJrRvYdZapqPQKrJ8BPWcxZVX0uIj5Qm5jNYmifyyC8T1c9xVxx6DFtSG4wM+VU7ba/XKdw+/wuY+FxgUvdnl4eMQCz3xCwrYsJop7rYIrkPJrhiuE42vHPiqvfAnwzMfDwyMW+MXHw8MjFvjFx8PDIxb4xcfDwyMW+MXHw8MjFvjFx8PDIxb4xcfDwyMWhOrngyA8G6sy063K0vZUuK7PgZ/LfrPOn2KZB++mbdkn1mVvtKtNfOlY58eDVLVPKY8dZlWMXTy2EXC8DHbhvy8dHDW/CTzz8fDwiAWhMh/e9V3Z0VxsJAyGsC5O56m/CQNxMZ7/q3CxmWXnls1PYBvWuoxx7Xrdp953neSwTgJwXWNfCHXx2fbhuoIWOVkS51SxH3oqlTJSYj41yjjudKv2sY1Ngxij7s827d7lO7vgKali7YT2nMOH5xxXc+AATDsNK+fz4e8+RRzcBssWNC5Njk/uF2CnS8Ux51qy+xI2vNjl4eERCyIJLLWp3rKcKSLBSo1dZjQaqUIUibBHo5HmjxmPx856S1BKctkWLnm7Ln9LGH20sSxtJ+9CrueB9qXT6ZWlXlz3W/X/bbFsZ8X/nmowiCp3zzaK3+l0aiTEFzETyHNVVn6uXE6ZMzOKBHOPcxZFXbbJxeLG47G2mz+5BpnNeDgHkF11VWQzaWJTRLL4rNLrcOlkkaBSADLtN5tNLZfMRc+4lhdy5CAtabVa1UjtarWqSZQKhcJCqksuMxtmH+2FRsSczI1GQ/vI1QXY8oTngsWzWq1qdYtqtar9jqoyggvLxBSuT75KDHaJH7x42TQ/DCsUt4XvN51OdR5xZRF8drtdHQ9+YXFNTsOay+WM+ugiZlKuYrGocy+KCPX5fK7t44oV/C7hk/sym80WksNx3uZsNqtpVNF++z0Kc+55scvDwyMWRGbt4nP4ezwe6+6DKhUPDw9yf38vIkHF0qurKxERub6+FhGRq6sr/U0ymdRk16ibdHJyoseDwUDq9bqImNUXwYay2WxoNJ+V41xBALtQr9fTfl1dXWm/UJes2WwqM5pOp7qjoK2vX7+W169f6/+xgxcKhYWKEGEyIDtTIbMctAXt7vf7+n9WwHKV0FXWF/5uPp9fqJywabvt58CVR9vttlaNZYaNc+12W32GmAUxU2Bmw8wbn/DfGQ6H+v9SqRRapRV+j1CLq9VqaR+4+gs+u92ufpfrx7nSqFarVSOZvEjgk4Q5yVVZw2B0nvl4eHjEgshM7cwQWD7F7vLw8CAiwS4EVnB3dyeXl5ciEtQ3FwlkV7CJdDqtsizv+pBJWRbnZNjMFHbdhdiEiX6NRiND7hYJdiTsPnd3dwbjEQl2WmZJuBbvPPjuwcGBsePYycvD2IVcrg2j0cio4om2Ygx6vZ6RTB5tYjaD8cBnoVDQsWMDgYhZQ4v/fgpcep7JZKK7frvdViba7XZ1fmE+NptNZRCsrwNSqZRRn902adv5qnke7jo+PDYiAbt2sZ3r62tjzqGvrFy3DRe5XE4Z22AwUIbr8u7niqdh6Ogit3axMnYymejE5gmOTkynUx000FlW9HHlA1aI8aLHFiJbLFnmBPaU/ti/sf0nXKEl+H86nZajoyMReax/NZ1OdaJcXV3pS4wJjj6ImCEG9rMNA1xyt9fr6ULT7XYXxJP7+3ttd7/fNyxEaDOXS8bExme9XtexRX9EgrFlCwv/7ylwWR8nk4mKT1zmeTgcLvi+JBIJo4QwjxOuxfPQvpddlSOKpOy82fGCgj6y+M/vCZ7nwcHBgsGH2z0ej41xBKJKnO/FLg8Pj1gQucKZwbsilFj2Kg6KCoVdLpfTHSmRSOjKjBWe62Ox2ZD9F8KivTbYJ4fbiPZhh08mkwsKzMFgoLvrfD5XhgHlX7lcdlb+tE3Zu4AZKZhXq9VSkbjVaqmoAgPA3d2dtnU0GukOiWfEviIij4pPML9EImFU1WSGsAuTc3nHz+dzfVaZTEbZ8mg00vECE0in03rMZaCBfr+vc4/FKnwvm80ajM9VpicMUV/EZPX5fF7fn0KhYEgMIsEYsykdYw7RmcVL7jeLy9zXL9bPZ1nUOiZoOp3WCYCBZqrIdBjX4hKu8/lcRQK8ADzBUqmU4XBoT6Aw6SM7nXEfAba2iYjqHgC2zrDYw1YttphE4WvB1hPWP7GFCIsHdCS3t7dGQT20kduHa7F/ExbSwWCgL7EtaoVtvePrFwoFQ9Sy/XjwPZHgxUVb+IVHX+bz+YJIzBtQmHXb+Z1iB1t+J1g0tP145vO5fjebzRoLKH7DFku7nDLPPRbRwoAXuzw8PGLBXsQuFk+wigKz2UyVmfP5XHdN7JTJZFJX5n6/r99lhRkYCHtj8u4VhcKMr+PaEViRPhwOlflA/Op0OvL27VsRCRTO6AP6fXJyoh7O5XLZELvC7gtfyw40RHvxyf4uR0dHcnx8LCKPNH86naqlhS1jYFM8BwqFgrEr20x1174w88nn84b/EsaDxWRmK3jOEEdZYZ1MJg3rnYjpD8MWvV1DEvi3LglCxLSCQTIAk2braDqd1nFg3x8gn88bfksiwdzDvZ4S5rMJIhG7llmVWOeD8/1+X+kwT1ae9GylwKIDVKtVo7Y4Wyxs3ciuegUbronBn2g3m9rRr8+fP8uHDx9EJLAqvXr1SkREXrx4ISIiz549U4dJpvHch7AcJtmEWiwWdeJ2Oh3DIVAkeMlOTk60rdDloN/dbldFteFwqJObRQZ21mOxZdd4KNfmhznAJv3BYKD3YodJlwMp5lur1dJzLIpA7GQHPBaNOaQnrM2CFz9uYyKRMDY3nMN9e72eLjoYo9FopONRKpV0zsE6aS+kYcasebHLw8MjFkRWLnmd8hmf4/FYHb1ubm6UsvMKy/4NYBNYmdmZ0LZwRR1Z7NrJ+Bx2pLu7O3n//r0ei4hcXl7qLlQqlVTEQkjF6emp7khhsIJV7eedlMMnXCIDW1RqtZo+b7BXDmNot9s6dmAK1WrVsMiwqGO3a1O4GAaLHJg78/nc8GsSCQwY+D1Yg8ijc+VwOFTrZCqVUhGLA0s5qp37Eraoz+IPGzX4XYLFcjqd6rlMJqOMB2OUz+fVsbVarSqTc7G4sBPleebj4eERC/aSUoPhCv5jj1pOayASrNxQ+rHLN6egwGrOKR6WtWtfaSg4J8xwOFR/mf/93/8VkUDng53y+PhYzs/PReQxYDafzy/151nnT/VU8E7qSm3B+iX2l8H/G43Ggm7k5uZGPn/+rP+3AzArlYruqqxgjmJsbB0FPzc7bxTrE/n/nMnQZXLmT3aBYJeRsPuVTCaNdgGc+wqMjl08hsPhQp6idDpt5CayMxnyfAib+UQmdrngCg0Yj8c60Cw2sULQVcEAi0+v11NR5vb2Vl/eQqEQGV1EX1ZhNpsZbu+YFFiE+v2+Uvdnz56p1Yj9MzDBwoijWQUWDeyXlX2oRILxgvIc1F7kcZLf3t5qfF6n05GXL1+KiBhiCo8L93EXxSy/5DzHXNfiBHYcN4W2cMgC/s/WJbbKcqI7XDOdThttiCLRm8uowz47vPFBjB4MBvo7l2GGFy9OXczjFWZfvNjl4eERC/bCfJalT8XfWFmZrfDOhNU8k8ks+J1wjqBWq6XH5XJ5YTffh9jF4HuBGcDkW6vV5O/+7u9EROTi4kIZDyuhXRHEYbsLuNrKSnsoWTmAFLt+JpPRHRbPvdPpqDIznU6ruAUTLrv4864aZvsZrgTxPOfYxYCZi80Q2LdmPB4rQ2CVAcaQmWQUY2WHkixL9Wqfy+VyqiDHPGTXim63q/3CJ4cshd2fvUa1z+dzw2dHRAy/nWKxuOBMx7mYOTkUxJdms2mEBnDuZ7Z0iGyXpGoZbJpvL6qcvrLT6ei9IRaWy2X5h3/4BxEJnPUw2AgbqVarRsyaa9BDpcD0rDDZarWaPm+c6/V6RigIu/aLBJMW7To8PFxwWuMxXuYPtm2/bJGe82Rz2hPWffCijsWlXC4vhE+w39ZwOFzQrbB4k0gkjAU8rNhC13tkx0baz65QKKhlOJVKGQ6DuBa/i7bzYT6fNzbxMDdyL3Z5eHjEgsjDK9ingoNI4XvAqThFzIA2kWCnxK47Ho+N9JC4JkdnsyLQthaF5b4PuHYiZjC//vqriAQWIOw+z58/FxGRH374Qb2DObCTU6uyyOBSCIcJDkkANZ9Op/oMITZ1u10jJxP79AAYw+fPnyvTA/Nhj9llyfDDjNbnfD5oa7fb1efNDIg9r3HM3sHcbzsvVa/Xc4Yh7Cqm8O+ZZbInM549BwUDXFShUqks+DJxTiY2kmAes7HHjtbfFZ75eHh4xILIU2rYKTPsBPLtdltX28lkYgQdijwyIFwL38Vqz7WI0um0wSBcHqdh9pHNmuzNLCLy5s0beffunf4fLAexW69fv9Zd//b2Vn/PqUZcqRJY1g41zsbSj4kEcUsYDw5IxDHvupxfCWzn9PRU48DYtOtioGF6AnPOJ+zgnU5H9WnNZtNIC4v7g+0cHBzoMa7V7XaNPrLORSTQl3C63zDnHM8JkWAswFwajYaRf8lOM3xwcKBzr1QqLaT7HY/HqoObTCYLaXOz2eyCu0VYiNzaxT4V3W5XJysocKPRMF48fBfnOKq93W5rXhl2LGSKzMo5FufChi1qoT8IFv306ZMqxUXEafXhCgQuZSmun8lkVJSIog4UPzdWKvLE4+RSvHFgnNBWdtev1+tG5RC0n+/FbdgGLuU0bwqcwxkvZqvVWvBnYREwlUoZPj8i5qLLoghvEK6yxK62bgoWI9EWzj3NvmOcEA2fLqU5R+tzRgLbmuzKfRQWvNjl4eERCyIPLLXN6y6xC6zBLp0sEuxS2HF4lceOW6lU1BelUCgYlQ9cRe+BMLIAYkcaDAaaZhQBpO/evdOdtlqtLuyK3Jdms6lsgbMbskJ7ndfurmCTM7fBDiNgRtnpdBaqV9RqNYPm865r38s+3qXdIoumds4Qyey03+8viLmcRpXLdmO+jkYjwxvbDpBmUdIOoHYxvU1gm9onk4nxHnG9MTschp/BeDzWZ8CKaVckAH5vi/lhzr3IrV0cNyQiCxSOfQu63e7CYLLOiMsL42U9PT2V09NTEQn8ZXghAp10lWLZlkK6nLvYkgKxkEuxjEYjIzYKbcGLKyJGEjT0357Q+AwzoZMLfH3bXZ/Hq9/v63nOKQxRi53TXM/e7kuY+W5wTX7hWZSyrVUi5gtri12TycQoncOF9ETMPNucp4gXqm3g0vGxmMz6Rl408ZtcLqfzkCPcAbbsFYtF7Q/6wgn6wi7V7cUuDw+PWBCZhzPAmfZ5FeWiavg/r+KAnQoT/jKg9s+ePdOI8OPjY2VE7EkbZmLyZTl8WCwRMb1gWRGInalareozODs7UwsRAkxLpZLB3FxR5/uAbQBgo0G/39f+cIUEVwpRVsyy/1UUwZZcEph9ilyVJjhMB4xuPB4vKPY5S2atVtP8SzAg1Ot1I/sfM59dPNHtaHn0j5+3S5nPSfw5cBtgCQF9OD4+1veKyyXzPAxzvDzz8fDwiAWR53Bmj1nOx8NKOK7jZCfAFnn0EanX67oyu9hOpVJxes+G6Q/jinNh8zIyEXLmO44bYjM0ds/T01O5uLjQ8+gz64ei1vMwOEWDrXjtdDqqSOdSvGyS59ppLkTVF9uTvVAoOHOB29/B/xmYk6z74LHDOOFcLpcLtSw398nOQ81llQaDgaFkxjhxvh8eG8xDfPI8PDo6MvojEi3rjkThbB9jMnLNcTy809NTDTlot9tGkByug8WrVqvpoOPFLhaLTprvovSbRoSvcpFPJB4Lx1WrVe0jFsfvvvvOcIS0a5aXSiWl6dVqdUHhvEokiTJimp8RO52x2IVz8/lcFa9c7JHrjdnWLp7AYVdDsBWzuVxOX6ZsNqttrdVq6nAIUYvDWXhseb7i9zxeHEAahfKc+4U5xptssVjUd+Lo6Ej7w6E5vPhwtQ2RYIxWpbWNcuPzYpeHh0csiDy8QsSkwzbdrdfrKqrseq+wmcK6dLDYUfL5vDIW7EIohfOUdm2KKPP5cK4bVnYyE2BvaxyDCbC5lutyuepzhc0Q7OeSTCYNRgkWxObrZbu67TMkYqoNXCEurmvx77ftq31dFm0rlYoaK57CUux+2e1b91zCRGROhk994fepw4jqvmG/RJvcVyTcxcheVG2ntVwuZzje2RYmLnDIBQbZIhPFxHYtKK6+Aev8VVwvqSuXjctnaVlbwuqvfS/X+WV9ilJk3xRe7PLw8IgFe08gD7jCBdYxlHVi3Sar+Zew8u+KqMQvtm5wpU8RU/TgY/49ELZC2YWn+tCwIp2P1z3DZXNylYhln9t0fm4SQrNMXFx17XXX2LYtm8IzHw8Pj1gQKvMJO1Ng3Ii64mkccJUh+poBs/63BC7T8y3j23u7PDw8vgr4xcfDwyMW+MXHw8MjFvjFx8PDIxb4xcfDwyMW+MXHw8MjFuwltutLxTrnKUQIu2A7X9l9D8Mxy+UU54rM53shnGEZ7NQRrusva8Oqc8va7wpDWHcvbhenYnXBLnu86v7Ljvl+6/rzlO+5fsO/W+eS4hqjVfffJVlZGNjWfcMzHw8Pj1iwF6/ATXbKbRC16/5T7rNs51117VVswg4SDCtAcdcIe7sU9lO//5TrbxoqYH930/FxXWOXe7n6ug828pQQpE2vsw945uPh4REL9sJ8nsoKuBYWZ9Pj0jlIRM55ZiBzLktfuWselaf8z1UzCmk7ufoo5PnhcGjI9nbeHM4RxCWGw0zfsUqfY2NZDTEbT2FrT9W3bMqWXOxxma6MM/3Z59YFMHOeI/6fS6cUJtal/+CSP/zJxy42z+WQ7dTDYVSVXYa9B2PZnbcLBXJ9KJGgQgJqUfd6Pf0uXtKDgwNNXlUulzU9ZDabXSiGFhWt5EHl0tBINzoej/UYCw73ZT6fGzl00H5XCs9dayety3tjv6Q8sUWChdSVkWCduOi65qq8NJv0x3XfVffnwofLSgGvy8djl0TmnN72M4xi3vEYYGz4/Vm2uNpzhxeX6XS6oBTPZDKRxTh6scvDwyMW7JX5MGXHCj0cDpUV9Pt9TbqOKouXl5dyeXkpIiIPDw/6e5iUnz17ptn3Ly4udKUvl8vKIraljqsYE/eFqzygVtdgMDCS4eM8+tXpdIxqmLgHGE4ul9OUrCy2lctlIyvgpn1aZup2MRPeVbHT22Wc8Ww5B5Brp3RRfxZfOMPhNkrqdXlzeLym06nRH/s3LrbD9bfm87mTMXG1XR6bsJi3S2zkuTEejw1R3+43XwNtmUwm2m4ujw3mM5lM1N2B63aFweb2au3iutFcFQH+NJ1OR25ubkRE5NOnTyIi8te//lU+f/4sIsGLiweFF7PVasmLFy/0+i76D93JslIuy7AuyRL3BYsPxMVer6fpRtvtti6qd3d3IhKIZViQuJwyBr1arRrlljGpXHXCXb4/q9q9yqrEL9Z0OjXERBFTB8eVETBBWSzENfBdG+l02sgN7Spr/VQ8RR/ChRsxTuiLLUbZvji86Np120WC/nNpG4jM6XR642Ri68CqislkovOo1+tpv/B+8Xd5AWWRH88glUoZ1VVEgs2OS0Ct87vaBF7s8vDwiAWRMx9bPGHLFc6BITw8PMj79+9FRORvf/ubiIh8+PBBbm9vRSTYnbAKY4VvNpu6y9gWIrtWVpiYzWZGQTq0B+fa7bbWhrq5uVGlOc51u13dfZiRYZdkNjWZTIwqEeivS/G7DuvErvF4bIiQKBB4f3+v/cI5VpQDzHy4fC++xwynVCoZuyqLLbsA/eJdn/vV6/Wk0Whof3DOVVaYRTVmFWgj14+D+H9ycuIUQ3cVVZh5MdPG+3N/f6/9YtYMcG0ytImfSzabVYMN+nV6emqwuzALCHrm4+HhEQv2qvNhFoQVuN/vqz7h5uZG9TtXV1ciEjAF7DL1el1XZKTPzGazKvM+PDxobabBYGCUxw27L8xGmAWB2dzf32sfbm5u9DyzBldJWugeut2uoYeADH54eKg77LawdRDz+dxgbMx2rq+vRUR0XO7v73W8ksmkUbETfWEXB2Y8IkGdKejr5vO5UcfNVv5uG0PFLI4NAGAFt7e3yrChg2s0Gnr/dDqt7AxGi/l8rixpMBho2zDfnj17pvcvFApqEAlDz4N2sY4R/Wo2mzpGnz59Up0pmI9tPmfdnIhpnk8kEsY8w/dwrlKphKa3EomxegVerOFwqIN6fX2tDxIPj8vBnpyc6DEmxXA41JeBr9XpdIx672GArT8iJg22FcpXV1dqpWu1WjpZuP4VKG6hUNDJgO+1223jJcdCdXJyYpT15c9t+iMSLKS4V7PZ1Al8eXkpHz9+FJFA/BV5XDzRB0xMfknRvsFgsFBUcDgcLixIy/qwyURPJBILlrnRaKTz6OHhQfvy9u1b+eWXX0Tkcbwmk4mOQa1W082NRV8WmdE2LomNPh4fH4f2kvLzxGba6/W0X58+fZJ3796JiMi7d+9UvMdvMpmMbtisLMYzYkvtaDTS+cf+dMfHx9qWMH2WvNjl4eERC/YeXoEVlykk6HCj0dCVGzg9PdVyyufn58p8cM12u60Mo91uG57RWNGZrWxC6dftXuhLp9PRHRSs4fr6WhWBg8FAdx3sQoeHh1KtVkUkoLMQVcAsPn78aLAgKM0bjcYCo9t0R7LDI9gDu9Pp6BhcXV0pW4DSfzKZKGOrVqu6K4IpZDIZZQh3d3d6DPC4sDKWXfs3YXSuMcKuPRwO9XleXl7KX/7yFxEJjBlv374VEZMhnJ+fi0jAXFCCGGym3W5r+/r9vj4vjHGn01H2yOOza9Anuz6gX4PBQOfZmzdv5NdffxWRYM7gfmh3pVKRo6MjEQnGC/MI1+r3+/r+TadT7RcYHTNwl7vELogtvAID1Ol0VJ9weXmpg3l6eioiIq9fv5aLiwsRETk7O9OJzwOJh8cWC/YpYt3Jpg55dttZZ4WBenh4WNCN3N3d6QCn02mVoTGpDw8PdSHlHDzQE93d3en9e72eTqZWq6WTYRvdz7J4LB4XdvRksUIkEEmg37i4uJDnz5+LyOPiM51OdTNg0RcT9/Dw0NAPQfRa5pz4VLDYBUwmE118Pnz4oOLJ+/fvdZ5hDC4uLuS3v/2tiIh8//33uqiirY1GQxesu7s7fS4Q1VjHmEwmF0TjMIBrdrtd3RTev3+vInGv15OTkxMREf18+fKljtfBwYGh6xEJ5i7mcbPZ1HnAujK2GHqxy8PD46vH3sMrbLHr/v5eKeTd3Z0RNiESMCDsqvl8XndHiFcc6GgrHW1X8zDAliIwkLu7O+0DxJPRaKRtPTo6UvaGHaleryubSaVSei2XyMFK9WazOBJzAQAADNJJREFUqX232/TU9tuiZyqVcnp/D4dDI6wDfYEY/Pr1a2VyUIj3ej0VQbndGNdEImF4AnNE9S6ezSx6si8K7n97e6tj0+l0tD1gbn/84x/ld7/7nYgEcw+KdDAEDiVxRYdzW2xPe/v/T4ErbIOV3GDaV1dXysJKpZIyObC4V69eqdhVKpX0vYPCut1uO6PiWWkPFsTMMgwG5JmPh4dHLNirnw8H9GG1/vjxo8ZxzWazBd1IpVJRZS2bGLGC39zcqFwPOR6w/SPCAO9urBvBror2JRIJ7cPz58/1GH4huVxOnwsHobLSks28rCDEfcPqSyqVMoIHebzYNUAkYD7YXSuVipFnCO2HDq7ZbOquCeaUy+WUyTLz4Wtsu6vaaSLS6bSyRPY4TyQS2obf//73IhIwBbDtcrms/eLgYIxtt9tdCKgtlUqG5zn7mO0aq2Y/l06nox7nDw8P+k7l83ll2Ih3PDk5URaXTCYXPPH7/b7BbHAtzIdUKmV4U4ep89n74oPOs6iFAa5Wq0qDMREqlYqh8MKLh4Xm9vZWrTOdTkcnDYta24pdrhwv7BuDyXhzc6M0GANVr9eV7h4fHxuLDvrCvk6s4MP1OfiPAzTR9219SVzR4yxScMIznOdASSzqrVZLxw7tu7m5UQXozc2N9gEiZrlc1pfBXuiW5dRZhXXiD57r7e2tLj7ZbFYXUIwRK/37/b72ERvb1dWVjvFgMNDngd9VKhVjbPFctsnrY+dJwjGecavV0sWn2WzqfQ8ODnSeoX2JRMIIomXxHdfCM0okEgtOoyLmQhVmbiwvdnl4eMSCvTKf8XisuzZYQ6PR0FX27OxMmQ9M6hzAxiEN7JWJa/V6PaX3LGrZfi0iuyc0Z6/mm5sb3VGwO3GQXi6XW9hJOSUst5d9XTgnDDMTW5G+aZCfi2Hw7znMwFZE93o9FZOvr6/VbwTjcXt7K2/evNHnAjGalczoCwcap9NpbZetYF2FdZkQMS5cBimZTKpYwYYPjE0ymdRnC4bw5s0b7XcqlVJmAaZRLBb1uQ2HQyOfzqYuBLZvkM18Op2OtouV32ziZ/Gdg7kxTnhnwKBEAiaK5+Jiwq5UIrtgL4sPHhrrA9Dp6XRqxMfgmCegK90oqHur1dKHb8epALtGSQNsTev3+yo6djodpa78kuF4Op3qZGGRhi09NtjvhfPecEjDtk5fqxbVRCLhTOWKNna7XUO3hvMYo4eHByMLAdrtWohHo5H+n0Xabai9K6fRdDo15gM/Q7Sb/YB442AnVpEgdAHfrVQq6iDKUe1sZcOcY9+wbXU/tm9cr9cz+sqiEuYh/MU4F9R0OtU55bJ2sR6S5x6OeezCCLXwYpeHh0csiIz52CKKSEBh4QOCz9lsZuwi2J2Y+WBln81myiY42I59fth3xnbX33W15j6xW/p4PNb28O6KnZQVmLwL8S7CiedFTO9gzuFTLpf1GSxr2zb94ayPYClnZ2e6Q3L+F7SVFZQsSuH/nFMJ7ecsf9v69jwVnHnv5ORE50mpVFKGjWfc7Xa1Lcx8wHYajYZhTUO/wUJrtZoqn4vFYmhJ112ZIGazmd6rXC6r5a5YLC5kaOQ0qZlMxghOxf/ZMse5lkSC8QK7C0uCADzz8fDwiAV7yWTIZnHoerCjZLNZ3cm53A1gm5xhwsR1Li8vVS7PZDK6I6TT6YVdOQywIpL1CegD7p9KpQwvUTt3Lsvy7L8ERthut/X3nPHv8PBQdyXGzvL3/39GpVJJfUSy2ayOEwc1upSOGINCoWCwHTAMsNt8Pq/jsm6MNmVzLuUzgkV//vlnNa+n02mN3QILY2VrLpfTOcvxgZx7CGPAbAdjxGb3XQNLXUHZ7JKSzWaV+RwcHBg17PB/zo3NecNFTEU855XiXM4cg/hV+flMp1MdyG63q6IKp6zklxCTnX0mcK7X66lPDwLrLi8v9VovXrwwoqztKg/bgicNWw44MRMnBsP30G+mvrwIscMl+yrh9xAh6/W69osTqm27qNovAVtkSqWSodTnyiIiwRgypbcLAqRSKV1w8vm8Wrv4JWWLCi9E9sTe1ILH/j1oC/zFcrmcLkRsSWRRgq2q2NwwX3me8oaJF5Nrq5VKJefis+2Li3mC35+enhqBvpgnnKKWFcfoK0f5Yzz7/b7+hsU5bBb1el2PC4WCDyz18PD4+hE582E/BPbqBbrdrpF+ADQQq/VgMDC8MXEMpjCfz+Xly5ciEqRFwO5Wq9UWSubY/ixPBdN5TvrOSj8wAHw2Gg3Du9dmPuwF2+12DforYppzT09PdQev1+uGeXpTMENwBS9ytrtMJqP94V2dk67b/kkcPlEsFvXYVb4ozITkLlN7Op02/MU47Ydt1h8Oh6qs7Xa7KgZjjNhUXygU9Lr4ZLGLTfn47bZghTPGpl6v67lKpWKIwXYqWvbT4dAc9JWrmJZKJe0P2Ovh4aGe+yrrdoHa5vN5I2ZEJHgIeCCNRmOh0gTneB6Px/pQIatfXFzI999/LyJBlDWixiuVyoKPish2PiQAO/uVy2UVhZrNpg4w1+pivRZb7NAOThIF6swJxrgaAkfDuyj9Jn1Y1S8On5jNZgsWDrsagp2lgOOicrncwqJjx5HtUqtrWd/wyTrA+XxuWII4Vk7ErLPWaDRU7OI0sPxCYiFjPx/cK5fLhaZn5A0Tc6RWqxm6Q9bj2GW5eYy63e5ChY5KpWLkRcecgzjJi49LNN4FXuzy8PCIBZEzn2QyaWTERyAf0z9YeG5vb438KSIB7cVqWyqVVPz48ccfRSTI1AZR6+TkxGAOts/QtmIXwB7WnEIUO4/II8tjkWQwGCzshOl02ghOxI6DcxwkyBUr2FoUVj0ovhYzk/n8sZoliynMYpjV2v/n8AxmOMx8XGKXSyxcBheTxTF7hrP4IvIoTrHCHMxnOBzqdzEGIo/ZGjkFLotibOAISzxhYwBYZLlcNkQtPMN2u21kUxRZzOLJoiE+MeeOjo60X5hvnFUibL+syBefVCqlHX727NlCSEA6ndaJyykdAZbVj4+P1Qz86tUrEQkc4Ti+hkMDdjW1uyY2y92s78B5LKS9Xk/7Ylt1RMwCh7lczjDTipjyN5es5X7tqvPh33MJZhbB7DiyZYm00CaOVGcHUZeDHrvu75pGw+UwyQspf4dThXBfMR6TyUTFXAaPPcYD882OiwrrJWWRmB1s+Xny2GEBZJEeY3hwcGBkiAAwz+r1uopbXJWEdaZhwotdHh4esSAy5sOrJFbs4+NjXVFB7y4uLjQIjgvSgSEVCgX97vn5+UKCbFZi26V67R1+05XblfeGHc2Q5J7d9bmsMNdAssWPbDarCkq2nrjKPTNLcjlibtOnZWAaP5lMFlz7OVhzMBgYbvr4PTNOMANXv1whMLtg1XhxUHChUND2csI5Zg12cT3uV7Va1XnMjI8ZSJgsgcVItJ9FX074ZueFWhZYypkL0Bdm2JibmUwmEmddEc98PDw8YkLkOh+WWfP5vO4uWGEvLi4ML1o25eL32D3ZO5YVmK6sb2F4lrp+z2Zc7Dj5fN5gciKm2XMymSzsyly6ltkGwHokW58Q9g5k635sj1o+thXKHMgLsHctGCHrrzgEYNdxWqVwRhsAVj7bbIIDatmAYDMFEZOJgjXsqotbBft6/NxSqZSRyZAzYYosZlVkL24AY8d+WRx2ws/wq1I4i5hiDyv4RExla1T3DQu2spaVlpjY7FfiUoayMpaPXXXKl4mQq9K7btsvvtaqxSWbzeridHh4uOBcyQ58nHQLlsFisbhghVzVnnVYNcb2gsR9wL25iB5vFna9MXbgKxQKRkUV+5pPbd+m4GuxEpiNGFhA2VeLlc9sOBAxle+cg8pOKhYFvNjl4eERC/ZesdSFXeh2GNd6KpZFKDPzcImA9vftv9nFna+57Frr2rYp7Hba7FTEZD6g7IVCwdhhRcydks3XLiXzunY/pU9P9Vi3WTeLLSKmaOuqT8XiZi6XW0i0vmqMwky6bl8nmUw6r8+GAo5wd6UUZuU1K83DbLMLX8Tisw2ifCjb3N/VnqeKD0+h6zzB7Mm2i8jlAutD7PAKW4RcdW+X4+Am9w/ju64F3Pb/EVneL5cuaVNn1SjGyfVcl12fFxxXu8NyiNwUXuzy8PCIBaEyn7jZSNjgWk7fCuwd/2tH2Kk9vwTYwdXfKjzz8fDwiAV+8fHw8IgFfvHx8PCIBX7x8fDwiAV+8fHw8IgFfvHx8PCIBX7x8fDwiAWJDYP4bkTkXXTN8fDw+Abxej6fL6SG3Gjx8fDw8AgLXuzy8PCIBX7x8fDwiAV+8fHw8IgFfvHx8PCIBX7x8fDwiAV+8fHw8IgFfvHx8PCIBX7x8fDwiAV+8fHw8IgF/w9tVmyWZ/k+cgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz_WVyh0gMdu"
      },
      "source": [
        "# BNN\n",
        "\n",
        "https://github.com/HIPS/autograd/blob/master/examples/bayesian_neural_net.py\n",
        "\n",
        "https://github.com/HIPS/autograd/blob/master/examples/black_box_svi.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iadc2_pGgIjO"
      },
      "source": [
        "import autograd.numpy as np\n",
        "import autograd.numpy.random as npr\n",
        "import autograd.scipy.stats.multivariate_normal as mvn\n",
        "import autograd.scipy.stats.norm as norm\n",
        "\n",
        "from autograd import grad\n",
        "from autograd.misc.optimizers import adam\n",
        "\n",
        "\n",
        "def black_box_variational_inference(logprob, D, num_samples):\n",
        "    \"\"\"Implements http://arxiv.org/abs/1401.0118, and uses the\n",
        "    local reparameterization trick from http://arxiv.org/abs/1506.02557\"\"\"\n",
        "\n",
        "    def unpack_params(params):\n",
        "        # Variational dist is a diagonal Gaussian.\n",
        "        mean, log_std = params[:D], params[D:]\n",
        "        return mean, log_std\n",
        "\n",
        "    def gaussian_entropy(log_std):\n",
        "        return 0.5 * D * (1.0 + np.log(2*np.pi)) + np.sum(log_std)\n",
        "\n",
        "    rs = npr.RandomState(0)\n",
        "    def variational_objective(params, t):\n",
        "        \"\"\"Provides a stochastic estimate of the variational lower bound.\"\"\"\n",
        "        mean, log_std = unpack_params(params)\n",
        "        samples = rs.randn(num_samples, D) * np.exp(log_std) + mean\n",
        "        lower_bound = gaussian_entropy(log_std) + np.mean(logprob(samples, t))\n",
        "        return -lower_bound\n",
        "\n",
        "    gradient = grad(variational_objective)\n",
        "\n",
        "    return variational_objective, gradient, unpack_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-WSH5o7gddA",
        "outputId": "a7250022-96f4-45af-a821-ca2a37ad950a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import autograd.numpy as np\n",
        "import autograd.numpy.random as npr\n",
        "\n",
        "#from black_box_svi import black_box_variational_inference\n",
        "from autograd.misc.optimizers import adam\n",
        "\n",
        "\n",
        "def make_nn_funs(layer_sizes, L2_reg, noise_variance, nonlinearity=np.tanh):\n",
        "    \"\"\"These functions implement a standard multi-layer perceptron,\n",
        "    vectorized over both training examples and weight samples.\"\"\"\n",
        "    shapes = list(zip(layer_sizes[:-1], layer_sizes[1:]))\n",
        "    num_weights = sum((m+1)*n for m, n in shapes)\n",
        "\n",
        "    def unpack_layers(weights):\n",
        "        num_weight_sets = len(weights)\n",
        "        for m, n in shapes:\n",
        "            yield weights[:, :m*n]     .reshape((num_weight_sets, m, n)),\\\n",
        "                  weights[:, m*n:m*n+n].reshape((num_weight_sets, 1, n))\n",
        "            weights = weights[:, (m+1)*n:]\n",
        "\n",
        "    def predictions(weights, inputs):\n",
        "        \"\"\"weights is shape (num_weight_samples x num_weights)\n",
        "           inputs  is shape (num_datapoints x D)\"\"\"\n",
        "        inputs = np.expand_dims(inputs, 0)\n",
        "        for W, b in unpack_layers(weights):\n",
        "            outputs = np.einsum('mnd,mdo->mno', inputs, W) + b\n",
        "            inputs = nonlinearity(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def logprob(weights, inputs, targets):\n",
        "        log_prior = -L2_reg * np.sum(weights**2, axis=1)\n",
        "        preds = predictions(weights, inputs)\n",
        "        log_lik = -np.sum((preds - targets)**2, axis=1)[:, 0] / noise_variance\n",
        "        return log_prior + log_lik\n",
        "\n",
        "    return num_weights, predictions, logprob\n",
        "\n",
        "\n",
        "def build_toy_dataset(n_data=40, noise_std=0.1):\n",
        "    D = 1\n",
        "    rs = npr.RandomState(0)\n",
        "    inputs  = np.concatenate([np.linspace(0, 2, num=n_data//2),\n",
        "                              np.linspace(6, 8, num=n_data//2)])\n",
        "    targets = np.cos(inputs) + rs.randn(n_data) * noise_std\n",
        "    inputs = (inputs - 4.0) / 4.0\n",
        "    inputs  = inputs.reshape((len(inputs), D))\n",
        "    targets = targets.reshape((len(targets), D))\n",
        "    return inputs, targets\n",
        "\n",
        "\n",
        "# Specify inference problem by its unnormalized log-posterior.\n",
        "rbf = lambda x: np.exp(-x**2)\n",
        "relu = lambda x: np.maximum(x, 0.)\n",
        "num_weights, predictions, logprob = \\\n",
        "    make_nn_funs(layer_sizes=[1, 20, 20, 1], L2_reg=0.1,\n",
        "                  noise_variance=0.01, nonlinearity=rbf)\n",
        "\n",
        "inputs, targets = build_toy_dataset()\n",
        "log_posterior = lambda weights, t: logprob(weights, inputs, targets)\n",
        "\n",
        "# Build variational objective.\n",
        "objective, gradient, unpack_params = \\\n",
        "    black_box_variational_inference(log_posterior, num_weights,\n",
        "                                    num_samples=20)\n",
        "\n",
        "# Set up figure.\n",
        "'''\n",
        "fig = plt.figure(figsize=(12, 8), facecolor='white')\n",
        "ax = fig.add_subplot(111, frameon=False)\n",
        "plt.ion()\n",
        "plt.show(block=False)\n",
        "'''\n",
        "\n",
        "\n",
        "def callback(params, t, g):\n",
        "    print(\"Iteration {} lower bound {}\".format(t, -objective(params, t)))\n",
        "\n",
        "    # Sample functions from posterior.\n",
        "    rs = npr.RandomState(0)\n",
        "    mean, log_std = unpack_params(params)\n",
        "    #rs = npr.RandomState(0)\n",
        "    sample_weights = rs.randn(10, num_weights) * np.exp(log_std) + mean\n",
        "    plot_inputs = np.linspace(-8, 8, num=400)\n",
        "    outputs = predictions(sample_weights, np.expand_dims(plot_inputs, 1))\n",
        "\n",
        "    # Plot data and functions.\n",
        "    '''\n",
        "    plt.cla()\n",
        "    ax.plot(inputs.ravel(), targets.ravel(), 'bx')\n",
        "    ax.plot(plot_inputs, outputs[:, :, 0].T)\n",
        "    ax.set_ylim([-2, 3])\n",
        "    plt.draw()\n",
        "    plt.pause(1.0/60.0)\n",
        "    '''\n",
        "\n",
        "# Initialize variational parameters\n",
        "rs = npr.RandomState(0)\n",
        "init_mean    = rs.randn(num_weights)\n",
        "init_log_std = -5 * np.ones(num_weights)\n",
        "init_var_params = np.concatenate([init_mean, init_log_std])\n",
        "\n",
        "print(\"Optimizing variational parameters...\")\n",
        "variational_params = adam(gradient, init_var_params,\n",
        "                          step_size=0.1, num_iters=1000, callback=callback)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizing variational parameters...\n",
            "Iteration 0 lower bound -11682.108060097273\n",
            "Iteration 1 lower bound -8370.240441279715\n",
            "Iteration 2 lower bound -4289.866187182439\n",
            "Iteration 3 lower bound -2469.3231150302317\n",
            "Iteration 4 lower bound -2331.5858920779633\n",
            "Iteration 5 lower bound -2394.769123796193\n",
            "Iteration 6 lower bound -2141.89016137708\n",
            "Iteration 7 lower bound -1937.5602411468362\n",
            "Iteration 8 lower bound -1834.4289964833297\n",
            "Iteration 9 lower bound -1715.5397033698605\n",
            "Iteration 10 lower bound -1617.0839382051636\n",
            "Iteration 11 lower bound -1687.452288928266\n",
            "Iteration 12 lower bound -1684.36164085515\n",
            "Iteration 13 lower bound -1615.6742108756744\n",
            "Iteration 14 lower bound -1486.5357137627225\n",
            "Iteration 15 lower bound -1424.2902985703417\n",
            "Iteration 16 lower bound -1373.926791874661\n",
            "Iteration 17 lower bound -1268.6318970168902\n",
            "Iteration 18 lower bound -1224.5002955852808\n",
            "Iteration 19 lower bound -1155.3426562622176\n",
            "Iteration 20 lower bound -1130.1205724709348\n",
            "Iteration 21 lower bound -1086.7374786708376\n",
            "Iteration 22 lower bound -1055.4490826538247\n",
            "Iteration 23 lower bound -1037.5364642755428\n",
            "Iteration 24 lower bound -935.3711077671776\n",
            "Iteration 25 lower bound -924.868416345445\n",
            "Iteration 26 lower bound -876.3517855981934\n",
            "Iteration 27 lower bound -827.5528336502836\n",
            "Iteration 28 lower bound -796.1451734338125\n",
            "Iteration 29 lower bound -740.3870242620349\n",
            "Iteration 30 lower bound -700.6256645047557\n",
            "Iteration 31 lower bound -642.8891583589367\n",
            "Iteration 32 lower bound -634.9382264677845\n",
            "Iteration 33 lower bound -594.5406241155858\n",
            "Iteration 34 lower bound -630.130500424523\n",
            "Iteration 35 lower bound -499.1759082129866\n",
            "Iteration 36 lower bound -471.6568004214455\n",
            "Iteration 37 lower bound -430.797917406739\n",
            "Iteration 38 lower bound -440.868917555612\n",
            "Iteration 39 lower bound -392.93414646321037\n",
            "Iteration 40 lower bound -323.9157946684643\n",
            "Iteration 41 lower bound -298.9534899326017\n",
            "Iteration 42 lower bound -285.88170947529073\n",
            "Iteration 43 lower bound -203.37455667566033\n",
            "Iteration 44 lower bound -203.46913234119512\n",
            "Iteration 45 lower bound -142.32658100076296\n",
            "Iteration 46 lower bound -94.89349713161957\n",
            "Iteration 47 lower bound -163.7746940934651\n",
            "Iteration 48 lower bound -89.23028899357692\n",
            "Iteration 49 lower bound -17.197901760026355\n",
            "Iteration 50 lower bound 16.36924278583001\n",
            "Iteration 51 lower bound 35.21721503750251\n",
            "Iteration 52 lower bound 20.014779911036726\n",
            "Iteration 53 lower bound -192.89099025479305\n",
            "Iteration 54 lower bound 21.238390406636654\n",
            "Iteration 55 lower bound -143.11884106868217\n",
            "Iteration 56 lower bound -1067.320041408871\n",
            "Iteration 57 lower bound -363.69725028567177\n",
            "Iteration 58 lower bound -109.80951131465321\n",
            "Iteration 59 lower bound -888.9069090865206\n",
            "Iteration 60 lower bound -270.03204426367677\n",
            "Iteration 61 lower bound -298.1567141035365\n",
            "Iteration 62 lower bound 89.17959444775175\n",
            "Iteration 63 lower bound -96.68729083115278\n",
            "Iteration 64 lower bound 3.205658516882238\n",
            "Iteration 65 lower bound 7.274623313756138\n",
            "Iteration 66 lower bound 16.505415514834965\n",
            "Iteration 67 lower bound -244.58403742572563\n",
            "Iteration 68 lower bound 3.4992495698778043\n",
            "Iteration 69 lower bound -2.6790881026718125\n",
            "Iteration 70 lower bound 55.01042912154708\n",
            "Iteration 71 lower bound 92.90677082922485\n",
            "Iteration 72 lower bound 120.46730741591216\n",
            "Iteration 73 lower bound 67.72753466321535\n",
            "Iteration 74 lower bound 42.08444678148089\n",
            "Iteration 75 lower bound 102.68331029308018\n",
            "Iteration 76 lower bound 85.59229258589369\n",
            "Iteration 77 lower bound 112.58839622067745\n",
            "Iteration 78 lower bound 137.93318171965944\n",
            "Iteration 79 lower bound -82.79498430363032\n",
            "Iteration 80 lower bound 76.67033228649495\n",
            "Iteration 81 lower bound 135.63491061919132\n",
            "Iteration 82 lower bound 96.71824097574535\n",
            "Iteration 83 lower bound 140.82873680750066\n",
            "Iteration 84 lower bound 140.51808983090712\n",
            "Iteration 85 lower bound 131.79880650173504\n",
            "Iteration 86 lower bound 89.93947976471281\n",
            "Iteration 87 lower bound 116.8114153642639\n",
            "Iteration 88 lower bound 135.83610226903738\n",
            "Iteration 89 lower bound 147.06509888110304\n",
            "Iteration 90 lower bound 139.38136666724284\n",
            "Iteration 91 lower bound 168.1490117894087\n",
            "Iteration 92 lower bound 146.12635639842057\n",
            "Iteration 93 lower bound 50.12142461098114\n",
            "Iteration 94 lower bound 22.917106439369377\n",
            "Iteration 95 lower bound -312.4774879429803\n",
            "Iteration 96 lower bound 178.4757672622543\n",
            "Iteration 97 lower bound -396.2422121222404\n",
            "Iteration 98 lower bound -1502.935686668744\n",
            "Iteration 99 lower bound 163.89863497350387\n",
            "Iteration 100 lower bound 175.42947001115442\n",
            "Iteration 101 lower bound 176.76915892380134\n",
            "Iteration 102 lower bound 112.8784202624596\n",
            "Iteration 103 lower bound 128.7865801866419\n",
            "Iteration 104 lower bound 149.4815310312734\n",
            "Iteration 105 lower bound 168.37738098943134\n",
            "Iteration 106 lower bound 161.58352151237432\n",
            "Iteration 107 lower bound 167.7413956843884\n",
            "Iteration 108 lower bound 167.78983858862352\n",
            "Iteration 109 lower bound 178.86239347563804\n",
            "Iteration 110 lower bound 145.56294782853735\n",
            "Iteration 111 lower bound 190.3114608144599\n",
            "Iteration 112 lower bound 181.67817919708438\n",
            "Iteration 113 lower bound 157.58897986643132\n",
            "Iteration 114 lower bound 179.2070710702547\n",
            "Iteration 115 lower bound 185.18575400185125\n",
            "Iteration 116 lower bound 174.25235734884495\n",
            "Iteration 117 lower bound 176.49056301822577\n",
            "Iteration 118 lower bound 199.005223754768\n",
            "Iteration 119 lower bound 58.691174588111664\n",
            "Iteration 120 lower bound 193.66360664530384\n",
            "Iteration 121 lower bound 181.97581223675348\n",
            "Iteration 122 lower bound 210.8749297728212\n",
            "Iteration 123 lower bound 184.58545158890922\n",
            "Iteration 124 lower bound 190.4736769679949\n",
            "Iteration 125 lower bound 105.62429503463494\n",
            "Iteration 126 lower bound 210.86494617094888\n",
            "Iteration 127 lower bound 185.88930133662933\n",
            "Iteration 128 lower bound 214.13740075977404\n",
            "Iteration 129 lower bound 220.17593133700655\n",
            "Iteration 130 lower bound 222.70238134334352\n",
            "Iteration 131 lower bound 232.0917621815571\n",
            "Iteration 132 lower bound 220.8933710029773\n",
            "Iteration 133 lower bound 159.80457521402332\n",
            "Iteration 134 lower bound 203.55732698101946\n",
            "Iteration 135 lower bound 230.41009257341614\n",
            "Iteration 136 lower bound 230.4504430230291\n",
            "Iteration 137 lower bound 203.62156823582865\n",
            "Iteration 138 lower bound 190.13739611639875\n",
            "Iteration 139 lower bound 241.97520410332106\n",
            "Iteration 140 lower bound 223.09692964846778\n",
            "Iteration 141 lower bound 220.22991121492515\n",
            "Iteration 142 lower bound 197.14576282734998\n",
            "Iteration 143 lower bound 229.49483607692991\n",
            "Iteration 144 lower bound 203.55976418243182\n",
            "Iteration 145 lower bound 243.6207823563734\n",
            "Iteration 146 lower bound 72.91942162623195\n",
            "Iteration 147 lower bound 248.98081125526204\n",
            "Iteration 148 lower bound 257.68112946087933\n",
            "Iteration 149 lower bound 237.13843621723822\n",
            "Iteration 150 lower bound 258.0452749717579\n",
            "Iteration 151 lower bound 254.30431767842663\n",
            "Iteration 152 lower bound 263.2938656801095\n",
            "Iteration 153 lower bound 257.0697925421701\n",
            "Iteration 154 lower bound 250.3521270626523\n",
            "Iteration 155 lower bound 256.296276063343\n",
            "Iteration 156 lower bound 263.88111788737115\n",
            "Iteration 157 lower bound 273.35892315990264\n",
            "Iteration 158 lower bound 265.26009877506084\n",
            "Iteration 159 lower bound 262.0392623038438\n",
            "Iteration 160 lower bound -92.29048695238362\n",
            "Iteration 161 lower bound 268.6393025400072\n",
            "Iteration 162 lower bound 281.7048251676778\n",
            "Iteration 163 lower bound 238.59941977248502\n",
            "Iteration 164 lower bound 270.1912571316055\n",
            "Iteration 165 lower bound 281.35305284967814\n",
            "Iteration 166 lower bound 245.40782151692792\n",
            "Iteration 167 lower bound 276.2853318419385\n",
            "Iteration 168 lower bound 261.0863706223085\n",
            "Iteration 169 lower bound 265.0510944192829\n",
            "Iteration 170 lower bound 285.69965284451655\n",
            "Iteration 171 lower bound 284.920341274363\n",
            "Iteration 172 lower bound 268.278725568274\n",
            "Iteration 173 lower bound 261.09723178086153\n",
            "Iteration 174 lower bound 285.6402855647506\n",
            "Iteration 175 lower bound 270.18730143356083\n",
            "Iteration 176 lower bound 271.4238797370776\n",
            "Iteration 177 lower bound 279.8052570720021\n",
            "Iteration 178 lower bound 288.8426290927996\n",
            "Iteration 179 lower bound 284.60257665924524\n",
            "Iteration 180 lower bound 288.92731882162616\n",
            "Iteration 181 lower bound 298.08943553219484\n",
            "Iteration 182 lower bound 272.1415463486413\n",
            "Iteration 183 lower bound 277.01128453356426\n",
            "Iteration 184 lower bound 298.5422425014355\n",
            "Iteration 185 lower bound 268.67200630732543\n",
            "Iteration 186 lower bound 302.57202364645786\n",
            "Iteration 187 lower bound 297.54598032877163\n",
            "Iteration 188 lower bound 280.9636614921421\n",
            "Iteration 189 lower bound 306.5738753691787\n",
            "Iteration 190 lower bound 308.1955994020228\n",
            "Iteration 191 lower bound 301.6900607979075\n",
            "Iteration 192 lower bound 296.9800093609898\n",
            "Iteration 193 lower bound 303.78308260977394\n",
            "Iteration 194 lower bound 306.1697694296507\n",
            "Iteration 195 lower bound 302.52785123442817\n",
            "Iteration 196 lower bound 300.8079713956959\n",
            "Iteration 197 lower bound 262.5508781490046\n",
            "Iteration 198 lower bound 295.9554956679062\n",
            "Iteration 199 lower bound 323.95278466014184\n",
            "Iteration 200 lower bound 304.3045004629482\n",
            "Iteration 201 lower bound 318.1199557874924\n",
            "Iteration 202 lower bound 329.8615594754842\n",
            "Iteration 203 lower bound 314.7223458154363\n",
            "Iteration 204 lower bound 270.3885714184686\n",
            "Iteration 205 lower bound 321.68148619386756\n",
            "Iteration 206 lower bound 304.97066380189\n",
            "Iteration 207 lower bound 330.91648122154487\n",
            "Iteration 208 lower bound 330.18038810005106\n",
            "Iteration 209 lower bound 320.3022529363247\n",
            "Iteration 210 lower bound 332.43797521521964\n",
            "Iteration 211 lower bound 310.8845937958235\n",
            "Iteration 212 lower bound 326.550126444821\n",
            "Iteration 213 lower bound 322.66789026817185\n",
            "Iteration 214 lower bound 320.22623375036704\n",
            "Iteration 215 lower bound 241.08121729513744\n",
            "Iteration 216 lower bound 342.2954613792683\n",
            "Iteration 217 lower bound 325.21120084069514\n",
            "Iteration 218 lower bound 333.61091792707464\n",
            "Iteration 219 lower bound 326.90621267758263\n",
            "Iteration 220 lower bound 339.827591395729\n",
            "Iteration 221 lower bound 328.6958038914523\n",
            "Iteration 222 lower bound 329.63123748761257\n",
            "Iteration 223 lower bound 315.69434473767694\n",
            "Iteration 224 lower bound 335.905814616021\n",
            "Iteration 225 lower bound 265.1984287555709\n",
            "Iteration 226 lower bound 333.70905583506305\n",
            "Iteration 227 lower bound 346.3726190471984\n",
            "Iteration 228 lower bound 336.63874361568946\n",
            "Iteration 229 lower bound 349.53536464978816\n",
            "Iteration 230 lower bound 342.9946821507873\n",
            "Iteration 231 lower bound 339.63206674269895\n",
            "Iteration 232 lower bound 354.15058963662614\n",
            "Iteration 233 lower bound 338.27507187355917\n",
            "Iteration 234 lower bound 354.32635843553\n",
            "Iteration 235 lower bound 350.3805376627237\n",
            "Iteration 236 lower bound 341.38387832700846\n",
            "Iteration 237 lower bound 368.9161112645415\n",
            "Iteration 238 lower bound 357.7440905432647\n",
            "Iteration 239 lower bound 240.72325360844115\n",
            "Iteration 240 lower bound 353.83179752214903\n",
            "Iteration 241 lower bound 357.3851031880252\n",
            "Iteration 242 lower bound 323.5205186580827\n",
            "Iteration 243 lower bound 347.28292699445535\n",
            "Iteration 244 lower bound 354.27644953608325\n",
            "Iteration 245 lower bound 352.1548817647012\n",
            "Iteration 246 lower bound 349.64595222748187\n",
            "Iteration 247 lower bound 340.96527555144417\n",
            "Iteration 248 lower bound 341.9382080210457\n",
            "Iteration 249 lower bound 366.56409215275664\n",
            "Iteration 250 lower bound 353.48882768845067\n",
            "Iteration 251 lower bound 333.5470295299234\n",
            "Iteration 252 lower bound 373.4411874581952\n",
            "Iteration 253 lower bound 382.2006049108289\n",
            "Iteration 254 lower bound 300.32797004208953\n",
            "Iteration 255 lower bound 367.07962645373254\n",
            "Iteration 256 lower bound 352.9096891469129\n",
            "Iteration 257 lower bound 372.57390084073677\n",
            "Iteration 258 lower bound 368.0597351369229\n",
            "Iteration 259 lower bound 340.38047464018064\n",
            "Iteration 260 lower bound 320.90502871173135\n",
            "Iteration 261 lower bound 334.89628764642123\n",
            "Iteration 262 lower bound 365.9008683212632\n",
            "Iteration 263 lower bound 382.279782302454\n",
            "Iteration 264 lower bound 368.48063319255164\n",
            "Iteration 265 lower bound 316.2722497766728\n",
            "Iteration 266 lower bound 365.43498253883837\n",
            "Iteration 267 lower bound 376.52373679554984\n",
            "Iteration 268 lower bound 374.1628985644015\n",
            "Iteration 269 lower bound 373.93873722533556\n",
            "Iteration 270 lower bound 362.5613289131741\n",
            "Iteration 271 lower bound 383.4420106283958\n",
            "Iteration 272 lower bound 388.1222673132187\n",
            "Iteration 273 lower bound 391.2404234492926\n",
            "Iteration 274 lower bound 378.5301923656318\n",
            "Iteration 275 lower bound 363.5523680884202\n",
            "Iteration 276 lower bound 380.99093947534715\n",
            "Iteration 277 lower bound 384.54426764024134\n",
            "Iteration 278 lower bound 391.12895717365865\n",
            "Iteration 279 lower bound 396.65761164812244\n",
            "Iteration 280 lower bound 398.0743207144544\n",
            "Iteration 281 lower bound 393.7580693082108\n",
            "Iteration 282 lower bound 414.7050994023486\n",
            "Iteration 283 lower bound 391.4266310521701\n",
            "Iteration 284 lower bound 403.78934797978843\n",
            "Iteration 285 lower bound 382.60163539066497\n",
            "Iteration 286 lower bound 400.05825827210504\n",
            "Iteration 287 lower bound 392.67747385448206\n",
            "Iteration 288 lower bound 393.1903230046879\n",
            "Iteration 289 lower bound 403.23114662682735\n",
            "Iteration 290 lower bound 377.89193635350017\n",
            "Iteration 291 lower bound 400.16221595175006\n",
            "Iteration 292 lower bound 378.430522061231\n",
            "Iteration 293 lower bound 405.3097540543689\n",
            "Iteration 294 lower bound 378.711134050898\n",
            "Iteration 295 lower bound 393.89910215225973\n",
            "Iteration 296 lower bound 243.21516603814854\n",
            "Iteration 297 lower bound 391.54787812430357\n",
            "Iteration 298 lower bound 407.557529133463\n",
            "Iteration 299 lower bound 395.89392174291834\n",
            "Iteration 300 lower bound 393.9867642789953\n",
            "Iteration 301 lower bound 407.1998914746845\n",
            "Iteration 302 lower bound 372.6303049636094\n",
            "Iteration 303 lower bound 367.19840134471656\n",
            "Iteration 304 lower bound 404.79933035264344\n",
            "Iteration 305 lower bound 398.14021818887903\n",
            "Iteration 306 lower bound 390.7661746097113\n",
            "Iteration 307 lower bound 381.95482950883803\n",
            "Iteration 308 lower bound 390.2433327402452\n",
            "Iteration 309 lower bound 396.49495806321346\n",
            "Iteration 310 lower bound 398.33967738348196\n",
            "Iteration 311 lower bound 399.42189214765204\n",
            "Iteration 312 lower bound 393.37946607003636\n",
            "Iteration 313 lower bound 396.46750077055526\n",
            "Iteration 314 lower bound 399.10287431877634\n",
            "Iteration 315 lower bound 403.4020176046593\n",
            "Iteration 316 lower bound 396.7012902509107\n",
            "Iteration 317 lower bound 378.714056587723\n",
            "Iteration 318 lower bound 388.8445991192082\n",
            "Iteration 319 lower bound 400.518203242023\n",
            "Iteration 320 lower bound 388.833181016215\n",
            "Iteration 321 lower bound 398.18436810268133\n",
            "Iteration 322 lower bound 403.69720372170855\n",
            "Iteration 323 lower bound 408.7652387153328\n",
            "Iteration 324 lower bound 412.9187841349561\n",
            "Iteration 325 lower bound 354.84156625118663\n",
            "Iteration 326 lower bound 416.38069804956814\n",
            "Iteration 327 lower bound 414.52385174653756\n",
            "Iteration 328 lower bound 416.83422482367047\n",
            "Iteration 329 lower bound 410.4912252227589\n",
            "Iteration 330 lower bound 425.78428513421625\n",
            "Iteration 331 lower bound 420.56520267800363\n",
            "Iteration 332 lower bound 379.53647534457696\n",
            "Iteration 333 lower bound 422.0302796780561\n",
            "Iteration 334 lower bound 434.7307421400871\n",
            "Iteration 335 lower bound 385.47485162484276\n",
            "Iteration 336 lower bound 426.1197651529152\n",
            "Iteration 337 lower bound 399.87107041759566\n",
            "Iteration 338 lower bound 416.0555077266184\n",
            "Iteration 339 lower bound 419.65012956865644\n",
            "Iteration 340 lower bound 407.88888605128426\n",
            "Iteration 341 lower bound 427.5819310229083\n",
            "Iteration 342 lower bound 397.25074381106623\n",
            "Iteration 343 lower bound 405.47125036251975\n",
            "Iteration 344 lower bound 442.1214350955188\n",
            "Iteration 345 lower bound 382.7125582575287\n",
            "Iteration 346 lower bound 363.1725791689849\n",
            "Iteration 347 lower bound 427.31795092811944\n",
            "Iteration 348 lower bound 378.6167026153873\n",
            "Iteration 349 lower bound 413.1244790159302\n",
            "Iteration 350 lower bound 395.0068847405166\n",
            "Iteration 351 lower bound 373.1756495679463\n",
            "Iteration 352 lower bound 409.09562894343793\n",
            "Iteration 353 lower bound 440.0536325370497\n",
            "Iteration 354 lower bound 419.90433015262386\n",
            "Iteration 355 lower bound 404.321770824923\n",
            "Iteration 356 lower bound 420.6089140019131\n",
            "Iteration 357 lower bound 424.6949524859189\n",
            "Iteration 358 lower bound 414.4336251574787\n",
            "Iteration 359 lower bound 286.90045012294263\n",
            "Iteration 360 lower bound 412.609363052071\n",
            "Iteration 361 lower bound 419.1818854762862\n",
            "Iteration 362 lower bound 407.66489300896495\n",
            "Iteration 363 lower bound 416.7183711229507\n",
            "Iteration 364 lower bound 379.3359583181495\n",
            "Iteration 365 lower bound 415.8407062529335\n",
            "Iteration 366 lower bound 427.19164688078354\n",
            "Iteration 367 lower bound 427.31842783523484\n",
            "Iteration 368 lower bound 432.90515279159246\n",
            "Iteration 369 lower bound 289.60737364347955\n",
            "Iteration 370 lower bound 416.78123325148556\n",
            "Iteration 371 lower bound 421.9795690673507\n",
            "Iteration 372 lower bound 413.8547903919675\n",
            "Iteration 373 lower bound 418.3729493919414\n",
            "Iteration 374 lower bound 417.1300535001991\n",
            "Iteration 375 lower bound 373.0600684647595\n",
            "Iteration 376 lower bound 416.2755581247026\n",
            "Iteration 377 lower bound 417.28205503705226\n",
            "Iteration 378 lower bound 430.4086094301381\n",
            "Iteration 379 lower bound 410.7212697923019\n",
            "Iteration 380 lower bound 420.3916249891897\n",
            "Iteration 381 lower bound 412.2769077095946\n",
            "Iteration 382 lower bound 405.89634362992564\n",
            "Iteration 383 lower bound 432.37792862541295\n",
            "Iteration 384 lower bound 408.0257596330352\n",
            "Iteration 385 lower bound 398.0378805050417\n",
            "Iteration 386 lower bound 419.37728552996566\n",
            "Iteration 387 lower bound 114.06610694957055\n",
            "Iteration 388 lower bound 407.4920101313186\n",
            "Iteration 389 lower bound 384.8293021040031\n",
            "Iteration 390 lower bound 418.1167927627175\n",
            "Iteration 391 lower bound 370.95462132422415\n",
            "Iteration 392 lower bound 377.6253018461007\n",
            "Iteration 393 lower bound 385.80353998237814\n",
            "Iteration 394 lower bound 401.6563602962476\n",
            "Iteration 395 lower bound 414.3931131250176\n",
            "Iteration 396 lower bound 396.39867938311534\n",
            "Iteration 397 lower bound 425.47278325173414\n",
            "Iteration 398 lower bound 413.03780994501653\n",
            "Iteration 399 lower bound 419.22334224886237\n",
            "Iteration 400 lower bound 401.3616697973228\n",
            "Iteration 401 lower bound 410.9504175441674\n",
            "Iteration 402 lower bound 423.5307292242319\n",
            "Iteration 403 lower bound 410.86017822986634\n",
            "Iteration 404 lower bound 411.8552511449511\n",
            "Iteration 405 lower bound 413.77755867303927\n",
            "Iteration 406 lower bound 418.37591128496047\n",
            "Iteration 407 lower bound 427.3559665330866\n",
            "Iteration 408 lower bound 414.01751778214094\n",
            "Iteration 409 lower bound 420.52426417565476\n",
            "Iteration 410 lower bound 432.919570631832\n",
            "Iteration 411 lower bound 431.42707822327765\n",
            "Iteration 412 lower bound 414.6043900729154\n",
            "Iteration 413 lower bound 430.6980766795091\n",
            "Iteration 414 lower bound 409.1492214712345\n",
            "Iteration 415 lower bound 431.4750015173612\n",
            "Iteration 416 lower bound 428.63181252781425\n",
            "Iteration 417 lower bound 401.3293797427585\n",
            "Iteration 418 lower bound 424.9785045371829\n",
            "Iteration 419 lower bound 428.70162335909674\n",
            "Iteration 420 lower bound 418.45593885947346\n",
            "Iteration 421 lower bound 430.8549891691061\n",
            "Iteration 422 lower bound 438.79542944272237\n",
            "Iteration 423 lower bound 431.8851388919113\n",
            "Iteration 424 lower bound 420.51929146840155\n",
            "Iteration 425 lower bound 421.19439646548307\n",
            "Iteration 426 lower bound 413.20379325750986\n",
            "Iteration 427 lower bound 438.93734945411705\n",
            "Iteration 428 lower bound 424.9696527099854\n",
            "Iteration 429 lower bound 442.26608860003387\n",
            "Iteration 430 lower bound 454.5689703935814\n",
            "Iteration 431 lower bound 408.25069650574403\n",
            "Iteration 432 lower bound 435.5156266878632\n",
            "Iteration 433 lower bound 442.9225103076579\n",
            "Iteration 434 lower bound 442.82639568055566\n",
            "Iteration 435 lower bound 456.20684864421065\n",
            "Iteration 436 lower bound 444.9744806916601\n",
            "Iteration 437 lower bound 427.05072404921077\n",
            "Iteration 438 lower bound 447.08327647852326\n",
            "Iteration 439 lower bound 437.03060730345544\n",
            "Iteration 440 lower bound 462.33733385848626\n",
            "Iteration 441 lower bound 440.50917993674517\n",
            "Iteration 442 lower bound 430.0510674074907\n",
            "Iteration 443 lower bound 420.873585066635\n",
            "Iteration 444 lower bound 447.2153559038962\n",
            "Iteration 445 lower bound 438.0223089827946\n",
            "Iteration 446 lower bound 434.72333366945367\n",
            "Iteration 447 lower bound 445.6604408081396\n",
            "Iteration 448 lower bound 427.0398228789259\n",
            "Iteration 449 lower bound 421.8621472486566\n",
            "Iteration 450 lower bound 410.4213426435652\n",
            "Iteration 451 lower bound 425.6493898048732\n",
            "Iteration 452 lower bound 434.5531188630202\n",
            "Iteration 453 lower bound 434.25666682285964\n",
            "Iteration 454 lower bound 444.0907810678302\n",
            "Iteration 455 lower bound 446.3051613953549\n",
            "Iteration 456 lower bound 419.40164276385275\n",
            "Iteration 457 lower bound 438.91246958409266\n",
            "Iteration 458 lower bound 435.68956995944455\n",
            "Iteration 459 lower bound 426.549397784045\n",
            "Iteration 460 lower bound 452.83077519241414\n",
            "Iteration 461 lower bound 420.56408634633874\n",
            "Iteration 462 lower bound 363.68290914742\n",
            "Iteration 463 lower bound 443.9142034007731\n",
            "Iteration 464 lower bound 443.30524188021235\n",
            "Iteration 465 lower bound 448.0262396809919\n",
            "Iteration 466 lower bound 448.83372088319663\n",
            "Iteration 467 lower bound 438.47348622159126\n",
            "Iteration 468 lower bound 467.4745786891709\n",
            "Iteration 469 lower bound 400.0214963552586\n",
            "Iteration 470 lower bound 402.1127284748044\n",
            "Iteration 471 lower bound 433.9583576186144\n",
            "Iteration 472 lower bound 429.68719055081834\n",
            "Iteration 473 lower bound 405.1053697213103\n",
            "Iteration 474 lower bound 452.36595280673055\n",
            "Iteration 475 lower bound 428.3318541600883\n",
            "Iteration 476 lower bound 414.90811186202876\n",
            "Iteration 477 lower bound 436.35706724057883\n",
            "Iteration 478 lower bound 418.20152418893247\n",
            "Iteration 479 lower bound 415.7004783981944\n",
            "Iteration 480 lower bound 439.7972919794921\n",
            "Iteration 481 lower bound 431.73828801393415\n",
            "Iteration 482 lower bound 423.55905971012027\n",
            "Iteration 483 lower bound 440.19159078239176\n",
            "Iteration 484 lower bound 435.5020630595154\n",
            "Iteration 485 lower bound 424.73632548698066\n",
            "Iteration 486 lower bound 310.4025406881258\n",
            "Iteration 487 lower bound 417.8711364407978\n",
            "Iteration 488 lower bound 397.90542433419967\n",
            "Iteration 489 lower bound 441.42922887543637\n",
            "Iteration 490 lower bound 412.5982013455122\n",
            "Iteration 491 lower bound 417.5435737432597\n",
            "Iteration 492 lower bound 439.44777292560394\n",
            "Iteration 493 lower bound 417.8871770228458\n",
            "Iteration 494 lower bound 417.56025959940644\n",
            "Iteration 495 lower bound 413.5833642104847\n",
            "Iteration 496 lower bound 396.28569528908986\n",
            "Iteration 497 lower bound 410.9000246036667\n",
            "Iteration 498 lower bound 412.11869902407307\n",
            "Iteration 499 lower bound 393.7986415019754\n",
            "Iteration 500 lower bound 393.6282456910002\n",
            "Iteration 501 lower bound 410.74914386964355\n",
            "Iteration 502 lower bound 406.9732404463552\n",
            "Iteration 503 lower bound 370.8030096041835\n",
            "Iteration 504 lower bound 417.76849199827825\n",
            "Iteration 505 lower bound 392.1956064721803\n",
            "Iteration 506 lower bound 406.0756733754126\n",
            "Iteration 507 lower bound 430.8697959759192\n",
            "Iteration 508 lower bound 425.72066771109684\n",
            "Iteration 509 lower bound 401.4404566265223\n",
            "Iteration 510 lower bound 396.2784047543939\n",
            "Iteration 511 lower bound 411.65974997905084\n",
            "Iteration 512 lower bound 424.458090880142\n",
            "Iteration 513 lower bound 404.29068095857104\n",
            "Iteration 514 lower bound 435.5668915548754\n",
            "Iteration 515 lower bound 433.8132613710962\n",
            "Iteration 516 lower bound 377.27029213958315\n",
            "Iteration 517 lower bound 433.95195055950063\n",
            "Iteration 518 lower bound 426.2637801241883\n",
            "Iteration 519 lower bound 441.8819142243308\n",
            "Iteration 520 lower bound 414.47124048189306\n",
            "Iteration 521 lower bound 430.0846714676464\n",
            "Iteration 522 lower bound 427.34705601076485\n",
            "Iteration 523 lower bound 443.7923170444922\n",
            "Iteration 524 lower bound 420.3782614426673\n",
            "Iteration 525 lower bound 434.25875902479174\n",
            "Iteration 526 lower bound 436.19347461264385\n",
            "Iteration 527 lower bound 388.12918190088624\n",
            "Iteration 528 lower bound 423.6249695841541\n",
            "Iteration 529 lower bound 419.0939587790482\n",
            "Iteration 530 lower bound 439.6526399516348\n",
            "Iteration 531 lower bound 432.6340240341227\n",
            "Iteration 532 lower bound 423.72184514597484\n",
            "Iteration 533 lower bound 419.63251222421843\n",
            "Iteration 534 lower bound 423.1769143146166\n",
            "Iteration 535 lower bound 414.88343747673275\n",
            "Iteration 536 lower bound 423.72564718584\n",
            "Iteration 537 lower bound 430.17529586734486\n",
            "Iteration 538 lower bound 421.5248700481418\n",
            "Iteration 539 lower bound 439.47102606445213\n",
            "Iteration 540 lower bound 427.30523432092644\n",
            "Iteration 541 lower bound 424.36592574858133\n",
            "Iteration 542 lower bound 427.1792892128016\n",
            "Iteration 543 lower bound 425.40972303324804\n",
            "Iteration 544 lower bound 427.8139448549572\n",
            "Iteration 545 lower bound 440.4844299562937\n",
            "Iteration 546 lower bound 444.6491092951254\n",
            "Iteration 547 lower bound 415.44072286360745\n",
            "Iteration 548 lower bound 401.7376514052841\n",
            "Iteration 549 lower bound 428.0103698156051\n",
            "Iteration 550 lower bound 434.63209526703497\n",
            "Iteration 551 lower bound 414.3090705641047\n",
            "Iteration 552 lower bound 419.27720830688236\n",
            "Iteration 553 lower bound 439.4682484376948\n",
            "Iteration 554 lower bound 429.62701148893154\n",
            "Iteration 555 lower bound 416.71473727143655\n",
            "Iteration 556 lower bound 416.50883488392014\n",
            "Iteration 557 lower bound 432.9125854622155\n",
            "Iteration 558 lower bound 437.7784511557645\n",
            "Iteration 559 lower bound 423.0020441232762\n",
            "Iteration 560 lower bound 433.83108941186447\n",
            "Iteration 561 lower bound 422.1072947098407\n",
            "Iteration 562 lower bound 439.8306895063336\n",
            "Iteration 563 lower bound 423.66815447248456\n",
            "Iteration 564 lower bound 415.5536208148209\n",
            "Iteration 565 lower bound 421.5814503554232\n",
            "Iteration 566 lower bound 430.33492197776457\n",
            "Iteration 567 lower bound 426.2868761687653\n",
            "Iteration 568 lower bound 433.39537059781065\n",
            "Iteration 569 lower bound 421.4322189022989\n",
            "Iteration 570 lower bound 434.5696177480089\n",
            "Iteration 571 lower bound 421.0897536123913\n",
            "Iteration 572 lower bound 430.0976892843766\n",
            "Iteration 573 lower bound 436.514698564313\n",
            "Iteration 574 lower bound 435.3142914807644\n",
            "Iteration 575 lower bound 431.7492515350024\n",
            "Iteration 576 lower bound 442.3686991253027\n",
            "Iteration 577 lower bound 420.7298544593371\n",
            "Iteration 578 lower bound 432.82823452008154\n",
            "Iteration 579 lower bound 441.8989960487447\n",
            "Iteration 580 lower bound 438.7762956371334\n",
            "Iteration 581 lower bound 444.0607902458152\n",
            "Iteration 582 lower bound 422.45326398327444\n",
            "Iteration 583 lower bound 450.9217699771407\n",
            "Iteration 584 lower bound 432.59367433985176\n",
            "Iteration 585 lower bound 442.09262187630935\n",
            "Iteration 586 lower bound 439.23912872935875\n",
            "Iteration 587 lower bound 443.3822480751063\n",
            "Iteration 588 lower bound 439.3433573514966\n",
            "Iteration 589 lower bound 442.96518427029514\n",
            "Iteration 590 lower bound 442.78440383334294\n",
            "Iteration 591 lower bound 374.39934128648883\n",
            "Iteration 592 lower bound 453.177621417\n",
            "Iteration 593 lower bound 430.9338423642365\n",
            "Iteration 594 lower bound 449.9961334117933\n",
            "Iteration 595 lower bound 427.5845693658688\n",
            "Iteration 596 lower bound 450.61907460272596\n",
            "Iteration 597 lower bound 455.52001923020595\n",
            "Iteration 598 lower bound 458.646098686611\n",
            "Iteration 599 lower bound 443.9548496584837\n",
            "Iteration 600 lower bound 449.2314656682617\n",
            "Iteration 601 lower bound 461.0017992483136\n",
            "Iteration 602 lower bound 445.4302457836622\n",
            "Iteration 603 lower bound 457.4924602805774\n",
            "Iteration 604 lower bound 454.61984163813446\n",
            "Iteration 605 lower bound 432.6854428745246\n",
            "Iteration 606 lower bound 427.32472742411187\n",
            "Iteration 607 lower bound 439.4691074251515\n",
            "Iteration 608 lower bound 436.9024465050194\n",
            "Iteration 609 lower bound 433.4580422307123\n",
            "Iteration 610 lower bound 451.078660278123\n",
            "Iteration 611 lower bound 447.6729759025392\n",
            "Iteration 612 lower bound 455.7335946932444\n",
            "Iteration 613 lower bound 461.1722300255198\n",
            "Iteration 614 lower bound 433.9635006464095\n",
            "Iteration 615 lower bound 446.6648262340848\n",
            "Iteration 616 lower bound 446.85121308090606\n",
            "Iteration 617 lower bound 452.550463469557\n",
            "Iteration 618 lower bound 452.4660946896106\n",
            "Iteration 619 lower bound 471.97911118718173\n",
            "Iteration 620 lower bound 440.7893923073831\n",
            "Iteration 621 lower bound 437.1700465434996\n",
            "Iteration 622 lower bound 456.05202229471166\n",
            "Iteration 623 lower bound 454.8306373987313\n",
            "Iteration 624 lower bound 458.9097666897037\n",
            "Iteration 625 lower bound 466.26638473637763\n",
            "Iteration 626 lower bound 458.3772171898635\n",
            "Iteration 627 lower bound 466.8204866988599\n",
            "Iteration 628 lower bound 444.7545781553621\n",
            "Iteration 629 lower bound 464.7952587813305\n",
            "Iteration 630 lower bound 458.07238209361935\n",
            "Iteration 631 lower bound 466.99227866469994\n",
            "Iteration 632 lower bound 417.1006637353174\n",
            "Iteration 633 lower bound 458.142120441034\n",
            "Iteration 634 lower bound 472.02737046380605\n",
            "Iteration 635 lower bound 456.66049486835345\n",
            "Iteration 636 lower bound 456.85914523842837\n",
            "Iteration 637 lower bound 464.6643655023049\n",
            "Iteration 638 lower bound 458.369423829819\n",
            "Iteration 639 lower bound 466.41523757456974\n",
            "Iteration 640 lower bound 471.1101525501294\n",
            "Iteration 641 lower bound 468.9673643606915\n",
            "Iteration 642 lower bound 464.31773262904494\n",
            "Iteration 643 lower bound 472.73875168736583\n",
            "Iteration 644 lower bound 459.62077868193415\n",
            "Iteration 645 lower bound 464.79692751062043\n",
            "Iteration 646 lower bound 463.7635600918996\n",
            "Iteration 647 lower bound 469.7732305642675\n",
            "Iteration 648 lower bound 464.4285096264473\n",
            "Iteration 649 lower bound 466.87724572875896\n",
            "Iteration 650 lower bound 458.4003423774978\n",
            "Iteration 651 lower bound 467.5633215712933\n",
            "Iteration 652 lower bound 452.81927224721954\n",
            "Iteration 653 lower bound 470.1159429963607\n",
            "Iteration 654 lower bound 470.94781571100617\n",
            "Iteration 655 lower bound 449.6843810459849\n",
            "Iteration 656 lower bound 475.7906085712483\n",
            "Iteration 657 lower bound 461.8963808246821\n",
            "Iteration 658 lower bound 407.14669237594427\n",
            "Iteration 659 lower bound 475.3567238494935\n",
            "Iteration 660 lower bound 463.0183129904999\n",
            "Iteration 661 lower bound 474.04830361507777\n",
            "Iteration 662 lower bound 475.9489982823357\n",
            "Iteration 663 lower bound 467.1245015018513\n",
            "Iteration 664 lower bound 471.0100623672995\n",
            "Iteration 665 lower bound 457.9187696408941\n",
            "Iteration 666 lower bound 459.6547623825969\n",
            "Iteration 667 lower bound 438.6101863727057\n",
            "Iteration 668 lower bound 464.45800211753874\n",
            "Iteration 669 lower bound 480.164677468919\n",
            "Iteration 670 lower bound 463.3273287416776\n",
            "Iteration 671 lower bound 477.46175162896225\n",
            "Iteration 672 lower bound 477.3361202959526\n",
            "Iteration 673 lower bound 459.7563539716709\n",
            "Iteration 674 lower bound 470.3302504435215\n",
            "Iteration 675 lower bound 470.25105522453225\n",
            "Iteration 676 lower bound 457.6804181997468\n",
            "Iteration 677 lower bound 474.2572663743299\n",
            "Iteration 678 lower bound 468.123473090094\n",
            "Iteration 679 lower bound 429.3098459707092\n",
            "Iteration 680 lower bound 475.46388513676914\n",
            "Iteration 681 lower bound 476.88690935086817\n",
            "Iteration 682 lower bound 465.41257493814294\n",
            "Iteration 683 lower bound 467.1417059596652\n",
            "Iteration 684 lower bound 476.21820015240405\n",
            "Iteration 685 lower bound 472.07136674113246\n",
            "Iteration 686 lower bound 469.48401106262924\n",
            "Iteration 687 lower bound 466.68466676612815\n",
            "Iteration 688 lower bound 485.87988419219096\n",
            "Iteration 689 lower bound 456.81107442966083\n",
            "Iteration 690 lower bound 473.19425184582843\n",
            "Iteration 691 lower bound 484.93445256437843\n",
            "Iteration 692 lower bound 447.86267238735667\n",
            "Iteration 693 lower bound 482.02652611236783\n",
            "Iteration 694 lower bound 467.4695867082329\n",
            "Iteration 695 lower bound 466.9463605878869\n",
            "Iteration 696 lower bound 478.0191701063703\n",
            "Iteration 697 lower bound 479.8810148151007\n",
            "Iteration 698 lower bound 462.06287953490255\n",
            "Iteration 699 lower bound 467.25738141969225\n",
            "Iteration 700 lower bound 480.41499093850825\n",
            "Iteration 701 lower bound 468.9797284914878\n",
            "Iteration 702 lower bound 472.4224565899785\n",
            "Iteration 703 lower bound 464.5441337420045\n",
            "Iteration 704 lower bound 464.2580451972394\n",
            "Iteration 705 lower bound 474.80722986115904\n",
            "Iteration 706 lower bound 473.24636652618267\n",
            "Iteration 707 lower bound 469.7340213932952\n",
            "Iteration 708 lower bound 467.03665657455997\n",
            "Iteration 709 lower bound 462.1177691584947\n",
            "Iteration 710 lower bound 470.425621287912\n",
            "Iteration 711 lower bound 472.8292961145147\n",
            "Iteration 712 lower bound 482.67480083964267\n",
            "Iteration 713 lower bound 472.10389304880516\n",
            "Iteration 714 lower bound 473.67159372364495\n",
            "Iteration 715 lower bound 476.1147113411486\n",
            "Iteration 716 lower bound 454.29479240275816\n",
            "Iteration 717 lower bound 470.40345982650445\n",
            "Iteration 718 lower bound 438.62851362078004\n",
            "Iteration 719 lower bound 440.4183706091361\n",
            "Iteration 720 lower bound 446.8014494354506\n",
            "Iteration 721 lower bound 462.2621787161005\n",
            "Iteration 722 lower bound 465.97444617352244\n",
            "Iteration 723 lower bound 465.59175511617826\n",
            "Iteration 724 lower bound 461.64217027259747\n",
            "Iteration 725 lower bound 466.5456703727092\n",
            "Iteration 726 lower bound 460.7058606502044\n",
            "Iteration 727 lower bound 470.49839647555814\n",
            "Iteration 728 lower bound 463.8993777899207\n",
            "Iteration 729 lower bound 461.535934725799\n",
            "Iteration 730 lower bound 472.36422967702777\n",
            "Iteration 731 lower bound 454.2305586959674\n",
            "Iteration 732 lower bound 454.8268429320719\n",
            "Iteration 733 lower bound 452.84157175580896\n",
            "Iteration 734 lower bound 423.23694575777563\n",
            "Iteration 735 lower bound 445.5156574142188\n",
            "Iteration 736 lower bound 443.5417706659132\n",
            "Iteration 737 lower bound 425.1793330561828\n",
            "Iteration 738 lower bound 468.5214314801846\n",
            "Iteration 739 lower bound 420.32256218501846\n",
            "Iteration 740 lower bound 406.1155558239208\n",
            "Iteration 741 lower bound 449.45869596241783\n",
            "Iteration 742 lower bound 427.61539919402065\n",
            "Iteration 743 lower bound 449.2990657294077\n",
            "Iteration 744 lower bound 436.01893522821774\n",
            "Iteration 745 lower bound 420.16780350081564\n",
            "Iteration 746 lower bound 455.4562647611299\n",
            "Iteration 747 lower bound 421.78312742733726\n",
            "Iteration 748 lower bound 466.44140286711684\n",
            "Iteration 749 lower bound 426.01100488211466\n",
            "Iteration 750 lower bound 458.45395952724243\n",
            "Iteration 751 lower bound 459.7233862269622\n",
            "Iteration 752 lower bound 439.3961732427039\n",
            "Iteration 753 lower bound 454.41814314703015\n",
            "Iteration 754 lower bound 444.1667772595044\n",
            "Iteration 755 lower bound 446.1699523290104\n",
            "Iteration 756 lower bound 431.0841161512044\n",
            "Iteration 757 lower bound 454.85813530634914\n",
            "Iteration 758 lower bound 421.8517349523191\n",
            "Iteration 759 lower bound 372.9338552327133\n",
            "Iteration 760 lower bound 334.5341425506671\n",
            "Iteration 761 lower bound 252.19192598516406\n",
            "Iteration 762 lower bound 344.9702938778279\n",
            "Iteration 763 lower bound 285.57338215005245\n",
            "Iteration 764 lower bound 322.2532270377594\n",
            "Iteration 765 lower bound 327.0846829166687\n",
            "Iteration 766 lower bound 339.9917919124995\n",
            "Iteration 767 lower bound 328.47579685425177\n",
            "Iteration 768 lower bound 363.24621007139854\n",
            "Iteration 769 lower bound 353.6384448490537\n",
            "Iteration 770 lower bound 389.54941966634834\n",
            "Iteration 771 lower bound 370.6676277668678\n",
            "Iteration 772 lower bound 382.41955704953773\n",
            "Iteration 773 lower bound 374.6989971117616\n",
            "Iteration 774 lower bound 382.26751694508096\n",
            "Iteration 775 lower bound 391.46818161959146\n",
            "Iteration 776 lower bound 391.42567172976425\n",
            "Iteration 777 lower bound 409.31284340510007\n",
            "Iteration 778 lower bound 409.65860677832444\n",
            "Iteration 779 lower bound 383.6539507694565\n",
            "Iteration 780 lower bound 405.6224233504772\n",
            "Iteration 781 lower bound 408.3285167740176\n",
            "Iteration 782 lower bound 416.66711400499634\n",
            "Iteration 783 lower bound 413.16561382085234\n",
            "Iteration 784 lower bound 420.9548635769712\n",
            "Iteration 785 lower bound 409.00204722384626\n",
            "Iteration 786 lower bound 404.6209462292198\n",
            "Iteration 787 lower bound 408.57474085796\n",
            "Iteration 788 lower bound 426.9970362547627\n",
            "Iteration 789 lower bound 402.37954837887276\n",
            "Iteration 790 lower bound 413.37555419751396\n",
            "Iteration 791 lower bound 418.2481142379971\n",
            "Iteration 792 lower bound 395.38227122045345\n",
            "Iteration 793 lower bound 418.31980695686593\n",
            "Iteration 794 lower bound 418.2930122665568\n",
            "Iteration 795 lower bound 421.683239335592\n",
            "Iteration 796 lower bound 407.38951475337603\n",
            "Iteration 797 lower bound 424.2328319090609\n",
            "Iteration 798 lower bound 421.61184219702875\n",
            "Iteration 799 lower bound 417.3707691993448\n",
            "Iteration 800 lower bound 423.6413356545753\n",
            "Iteration 801 lower bound 417.3956288417829\n",
            "Iteration 802 lower bound 439.3510514186061\n",
            "Iteration 803 lower bound 436.920498854528\n",
            "Iteration 804 lower bound 432.37809271940284\n",
            "Iteration 805 lower bound 432.75717602225967\n",
            "Iteration 806 lower bound 428.1830000014828\n",
            "Iteration 807 lower bound 427.39078602911843\n",
            "Iteration 808 lower bound 426.17453773783325\n",
            "Iteration 809 lower bound 428.1699401689975\n",
            "Iteration 810 lower bound 441.1237513108886\n",
            "Iteration 811 lower bound 437.36090495380347\n",
            "Iteration 812 lower bound 440.4959116578451\n",
            "Iteration 813 lower bound 444.7596614084227\n",
            "Iteration 814 lower bound 438.47133143063144\n",
            "Iteration 815 lower bound 437.0277303024282\n",
            "Iteration 816 lower bound 439.3049966919644\n",
            "Iteration 817 lower bound 445.76581734827977\n",
            "Iteration 818 lower bound 443.4860299235217\n",
            "Iteration 819 lower bound 442.7489844896925\n",
            "Iteration 820 lower bound 437.3363838918841\n",
            "Iteration 821 lower bound 452.7062978600945\n",
            "Iteration 822 lower bound 440.1194923684049\n",
            "Iteration 823 lower bound 436.125206737842\n",
            "Iteration 824 lower bound 442.5153662685866\n",
            "Iteration 825 lower bound 432.8749265128954\n",
            "Iteration 826 lower bound 439.83248002131864\n",
            "Iteration 827 lower bound 429.6214051627479\n",
            "Iteration 828 lower bound 441.6159603524429\n",
            "Iteration 829 lower bound 441.69322616579075\n",
            "Iteration 830 lower bound 432.7282700954917\n",
            "Iteration 831 lower bound 411.5456712192404\n",
            "Iteration 832 lower bound 453.1156649276817\n",
            "Iteration 833 lower bound 429.2825149021189\n",
            "Iteration 834 lower bound 431.2852922580941\n",
            "Iteration 835 lower bound 437.40238086652346\n",
            "Iteration 836 lower bound 441.8467240401461\n",
            "Iteration 837 lower bound 438.6522854875209\n",
            "Iteration 838 lower bound 446.49312239100584\n",
            "Iteration 839 lower bound 409.61484771061475\n",
            "Iteration 840 lower bound 422.1365798543462\n",
            "Iteration 841 lower bound 444.0305516487968\n",
            "Iteration 842 lower bound 433.00670784879446\n",
            "Iteration 843 lower bound 441.98005151323673\n",
            "Iteration 844 lower bound 448.1654375275275\n",
            "Iteration 845 lower bound 421.1603625942349\n",
            "Iteration 846 lower bound 440.98304371882136\n",
            "Iteration 847 lower bound 442.01839942545604\n",
            "Iteration 848 lower bound 439.2954721155048\n",
            "Iteration 849 lower bound 434.8003778805923\n",
            "Iteration 850 lower bound 438.40315839770744\n",
            "Iteration 851 lower bound 441.56497633855196\n",
            "Iteration 852 lower bound 451.3238824480047\n",
            "Iteration 853 lower bound 441.4982539168631\n",
            "Iteration 854 lower bound 450.6996429129809\n",
            "Iteration 855 lower bound 429.81476153953133\n",
            "Iteration 856 lower bound 446.63249015646824\n",
            "Iteration 857 lower bound 438.96795662029854\n",
            "Iteration 858 lower bound 440.2493406277043\n",
            "Iteration 859 lower bound 440.61975676414215\n",
            "Iteration 860 lower bound 455.3224082094341\n",
            "Iteration 861 lower bound 425.68287628468164\n",
            "Iteration 862 lower bound 455.0189472990691\n",
            "Iteration 863 lower bound 453.8081593582656\n",
            "Iteration 864 lower bound 446.47545981760805\n",
            "Iteration 865 lower bound 457.87854340965083\n",
            "Iteration 866 lower bound 452.45108933771536\n",
            "Iteration 867 lower bound 456.8133566067621\n",
            "Iteration 868 lower bound 436.4236718052082\n",
            "Iteration 869 lower bound 449.36479962064476\n",
            "Iteration 870 lower bound 453.77560488506526\n",
            "Iteration 871 lower bound 465.1556870634999\n",
            "Iteration 872 lower bound 444.58995165739475\n",
            "Iteration 873 lower bound 442.8518121793967\n",
            "Iteration 874 lower bound 425.2339270418557\n",
            "Iteration 875 lower bound 462.51053614493765\n",
            "Iteration 876 lower bound 450.26879359636814\n",
            "Iteration 877 lower bound 441.15936787672825\n",
            "Iteration 878 lower bound 465.06000437298576\n",
            "Iteration 879 lower bound 457.45455343521206\n",
            "Iteration 880 lower bound 436.4112352101686\n",
            "Iteration 881 lower bound 453.4371275676127\n",
            "Iteration 882 lower bound 449.85924453756627\n",
            "Iteration 883 lower bound 449.9616322366323\n",
            "Iteration 884 lower bound 443.35230613857715\n",
            "Iteration 885 lower bound 468.68289445017103\n",
            "Iteration 886 lower bound 455.2064411213447\n",
            "Iteration 887 lower bound 453.41137697861365\n",
            "Iteration 888 lower bound 450.68636676009817\n",
            "Iteration 889 lower bound 451.62341933557923\n",
            "Iteration 890 lower bound 453.97397352241427\n",
            "Iteration 891 lower bound 457.98110234151727\n",
            "Iteration 892 lower bound 461.32096361053334\n",
            "Iteration 893 lower bound 449.6827587170042\n",
            "Iteration 894 lower bound 461.8865468936038\n",
            "Iteration 895 lower bound 450.52214245639044\n",
            "Iteration 896 lower bound 439.24230252736504\n",
            "Iteration 897 lower bound 462.34128945236574\n",
            "Iteration 898 lower bound 460.1502145571298\n",
            "Iteration 899 lower bound 463.4605543655332\n",
            "Iteration 900 lower bound 328.3886318800559\n",
            "Iteration 901 lower bound 459.05087930528254\n",
            "Iteration 902 lower bound 461.86950819421514\n",
            "Iteration 903 lower bound 447.22976070128436\n",
            "Iteration 904 lower bound 468.0413234057432\n",
            "Iteration 905 lower bound 465.91656609071373\n",
            "Iteration 906 lower bound 456.8796174605556\n",
            "Iteration 907 lower bound 408.6366909189816\n",
            "Iteration 908 lower bound 405.7283947451131\n",
            "Iteration 909 lower bound 468.3109800922992\n",
            "Iteration 910 lower bound 470.45282784858796\n",
            "Iteration 911 lower bound 450.1901192426508\n",
            "Iteration 912 lower bound 481.7638174826368\n",
            "Iteration 913 lower bound 469.7618857467015\n",
            "Iteration 914 lower bound 468.7234413847393\n",
            "Iteration 915 lower bound 469.4377402230617\n",
            "Iteration 916 lower bound 443.0074758312237\n",
            "Iteration 917 lower bound 425.667303427983\n",
            "Iteration 918 lower bound 438.12787123256334\n",
            "Iteration 919 lower bound 365.7925425892461\n",
            "Iteration 920 lower bound 434.45066682107347\n",
            "Iteration 921 lower bound 434.03518551994205\n",
            "Iteration 922 lower bound 469.64908173339865\n",
            "Iteration 923 lower bound 462.14910036304207\n",
            "Iteration 924 lower bound 459.38061162308935\n",
            "Iteration 925 lower bound 466.8903817324752\n",
            "Iteration 926 lower bound 462.296194335755\n",
            "Iteration 927 lower bound 463.0617716835869\n",
            "Iteration 928 lower bound 452.02722164063647\n",
            "Iteration 929 lower bound 441.6485932336343\n",
            "Iteration 930 lower bound 462.2998489534461\n",
            "Iteration 931 lower bound 455.7978609546417\n",
            "Iteration 932 lower bound 456.8451191664544\n",
            "Iteration 933 lower bound 459.7297165647796\n",
            "Iteration 934 lower bound 469.66985489449127\n",
            "Iteration 935 lower bound 430.0391886385456\n",
            "Iteration 936 lower bound 436.19728051288746\n",
            "Iteration 937 lower bound 453.47652223134753\n",
            "Iteration 938 lower bound 439.6047381016984\n",
            "Iteration 939 lower bound 438.3929704045159\n",
            "Iteration 940 lower bound 455.3961848763573\n",
            "Iteration 941 lower bound 432.8665977047897\n",
            "Iteration 942 lower bound 470.2286937323222\n",
            "Iteration 943 lower bound 427.5085994735988\n",
            "Iteration 944 lower bound 463.3828609027461\n",
            "Iteration 945 lower bound 466.807540979743\n",
            "Iteration 946 lower bound 421.6792023084779\n",
            "Iteration 947 lower bound 473.61275418696187\n",
            "Iteration 948 lower bound 449.4236488631606\n",
            "Iteration 949 lower bound 417.14637690950485\n",
            "Iteration 950 lower bound 466.6227485304808\n",
            "Iteration 951 lower bound 457.58640371536495\n",
            "Iteration 952 lower bound 456.82775363580754\n",
            "Iteration 953 lower bound 467.66628801337293\n",
            "Iteration 954 lower bound 456.377824620639\n",
            "Iteration 955 lower bound 461.69796266717213\n",
            "Iteration 956 lower bound 452.69368241080946\n",
            "Iteration 957 lower bound 458.8163607211886\n",
            "Iteration 958 lower bound 453.8130467420891\n",
            "Iteration 959 lower bound 456.08157680595986\n",
            "Iteration 960 lower bound 457.9736981816887\n",
            "Iteration 961 lower bound 466.4041295971385\n",
            "Iteration 962 lower bound 463.5880470724758\n",
            "Iteration 963 lower bound 466.5721469861066\n",
            "Iteration 964 lower bound 473.2253585129103\n",
            "Iteration 965 lower bound 462.7991461553181\n",
            "Iteration 966 lower bound 476.0921468792238\n",
            "Iteration 967 lower bound 466.79096039505606\n",
            "Iteration 968 lower bound 446.04058902536116\n",
            "Iteration 969 lower bound 473.1927193875116\n",
            "Iteration 970 lower bound 460.29922455058534\n",
            "Iteration 971 lower bound 456.9570529972375\n",
            "Iteration 972 lower bound 425.59272986615304\n",
            "Iteration 973 lower bound 464.0588404947383\n",
            "Iteration 974 lower bound 462.32707061563246\n",
            "Iteration 975 lower bound 453.5281002025657\n",
            "Iteration 976 lower bound 453.59864501366434\n",
            "Iteration 977 lower bound 451.1108312115674\n",
            "Iteration 978 lower bound 350.416496912403\n",
            "Iteration 979 lower bound 393.4841665644201\n",
            "Iteration 980 lower bound 432.6438147578033\n",
            "Iteration 981 lower bound 431.0941284558085\n",
            "Iteration 982 lower bound 437.7983844438327\n",
            "Iteration 983 lower bound 446.4447107647649\n",
            "Iteration 984 lower bound 426.1357295025048\n",
            "Iteration 985 lower bound 440.3838606132382\n",
            "Iteration 986 lower bound 436.2795785084865\n",
            "Iteration 987 lower bound 409.013480903615\n",
            "Iteration 988 lower bound 450.2250522402156\n",
            "Iteration 989 lower bound 416.28966704574145\n",
            "Iteration 990 lower bound 412.36677462825685\n",
            "Iteration 991 lower bound 428.2633033938478\n",
            "Iteration 992 lower bound 415.9360393095537\n",
            "Iteration 993 lower bound 433.39540784002213\n",
            "Iteration 994 lower bound 425.6321100173906\n",
            "Iteration 995 lower bound 442.96720333334736\n",
            "Iteration 996 lower bound 438.8480788334862\n",
            "Iteration 997 lower bound 410.0190350170084\n",
            "Iteration 998 lower bound 414.5690377166777\n",
            "Iteration 999 lower bound 432.50367606047956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrUs7VOoaY__"
      },
      "source": [
        "# Conditional VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWLYPQh9a0Z-"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WacMYv7j_TN4",
        "outputId": "44587abd-da37-4cea-b405-725e6efbb78f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "import autograd.numpy as np\n",
        "import autograd.numpy.random as npr\n",
        "import autograd.scipy.stats.norm as norm\n",
        "from autograd.scipy.special import expit as sigmoid\n",
        "\n",
        "from autograd import grad\n",
        "from autograd.misc.optimizers import adam\n",
        "#from data import load_mnist, save_images\n",
        "\n",
        "def diag_gaussian_log_density(x, mu, log_std):\n",
        "    return np.sum(norm.logpdf(x, mu, np.exp(log_std)), axis=-1) # https://stackoverflow.com/questions/43602270/what-is-probability-density-function-in-the-context-of-scipy-stats-norm\n",
        "\n",
        "def unpack_gaussian_params(params):\n",
        "    # Params of a diagonal Gaussian.\n",
        "    D = np.shape(params)[-1] // 2\n",
        "    mean, log_std = params[:, :D], params[:, D:]\n",
        "    return mean, log_std\n",
        "\n",
        "def sample_diag_gaussian(mean, log_std, rs):\n",
        "    return rs.randn(*mean.shape) * np.exp(log_std) + mean\n",
        "\n",
        "def bernoulli_log_density(targets, unnormalized_logprobs):\n",
        "    # unnormalized_logprobs are in R\n",
        "    # Targets must be -1 or 1\n",
        "    label_probabilities = -np.logaddexp(0, -unnormalized_logprobs*targets)  # Logarithm of exp(x1) + exp(x2), works out to the logistic formula\n",
        "    return np.sum(label_probabilities, axis=-1)   # Sum across pixels.\n",
        "\n",
        "def relu(x):    return np.maximum(0, x)\n",
        "\n",
        "def init_net_params(scale, layer_sizes, rs=npr.RandomState(0)):\n",
        "    \"\"\"Build a (weights, biases) tuples for all layers.\"\"\"\n",
        "    return [(scale * rs.randn(m, n),   # weight matrix\n",
        "             scale * rs.randn(n))      # bias vector\n",
        "            for m, n in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
        "\n",
        "def batch_normalize(activations):\n",
        "    mbmean = np.mean(activations, axis=0, keepdims=True)\n",
        "    return (activations - mbmean) / (np.std(activations, axis=0, keepdims=True) + 1)\n",
        "\n",
        "def neural_net_predict(params, inputs, labels):\n",
        "    \"\"\"Params is a list of (weights, bias) tuples.\n",
        "       inputs is an (N x D) matrix.\n",
        "       Applies batch normalization to every layer but the last.\"\"\"\n",
        "    for i, (W, b) in enumerate(params[:-1]):\n",
        "        #print(f'inputs:{inputs.shape}')\n",
        "        #print(f'labels:{labels.shape}')\n",
        "        if i == 0:\n",
        "          inputs = np.concatenate((inputs, labels), axis=1)\n",
        "        outputs = batch_normalize(np.dot(inputs, W) + b)  # linear transformation\n",
        "        inputs = relu(outputs)                            # nonlinear transformation\n",
        "    outW, outb = params[-1]\n",
        "    outputs = np.dot(inputs, outW) + outb\n",
        "    return outputs\n",
        "\n",
        "def nn_predict_gaussian(params, inputs, labels):\n",
        "    # Returns means and diagonal variances\n",
        "    return unpack_gaussian_params(neural_net_predict(params, inputs, labels))\n",
        "\n",
        "def generate_from_prior(gen_params, num_samples, noise_dim, labels, rs):\n",
        "    latents = rs.randn(num_samples, noise_dim)\n",
        "    return sigmoid(neural_net_predict(gen_params, latents, labels))\n",
        "\n",
        "def p_images_given_latents(gen_params, images, labels, latents):\n",
        "    preds = neural_net_predict(gen_params, latents, labels)\n",
        "    return bernoulli_log_density(images, preds)\n",
        "\n",
        "def vae_lower_bound(gen_params, rec_params, data, labels, rs):\n",
        "    # We use a simple Monte Carlo estimate of the KL\n",
        "    # divergence from the prior.\n",
        "    q_means, q_log_stds = nn_predict_gaussian(rec_params, data, labels)\n",
        "    latents = sample_diag_gaussian(q_means, q_log_stds, rs)\n",
        "    q_latents = diag_gaussian_log_density(latents, q_means, q_log_stds)\n",
        "    p_latents = diag_gaussian_log_density(latents, 0, 0) # standard normal prior\n",
        "    likelihood = p_images_given_latents(gen_params, data, labels, latents)\n",
        "    return np.mean(p_latents + likelihood - q_latents) #elbow\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "\n",
        "\n",
        "# Model hyper-parameters\n",
        "latent_dim = 10\n",
        "data_dim = 784  # How many pixels in each image (28x28).\n",
        "cond_dim = 10 # conditioning dimension (1 hot vector)\n",
        "\n",
        "gen_layer_sizes = [latent_dim + cond_dim, 300, 200, data_dim] # latent, hidden_3, hidden_4, output\n",
        "rec_layer_sizes = [data_dim + cond_dim, 200, 300, latent_dim * 2] # input, hidden_1, hidden_2, latent (mu and sigma)\n",
        "\n",
        "# Training parameters\n",
        "param_scale = 0.01\n",
        "batch_size = 200\n",
        "#num_epochs = 15\n",
        "num_epochs = 20\n",
        "step_size = 0.001\n",
        "\n",
        "print(\"Loading training data...\")\n",
        "N, train_images, train_labels, test_images, test_labels = load_mnist()\n",
        "# -1 or 1 ?\n",
        "def binarise(images):\n",
        "    on = images > 0.5\n",
        "    images = images * 0 - 1\n",
        "    images[on] = 1.0\n",
        "    return images\n",
        "\n",
        "print(\"Binarising training data...\")\n",
        "train_images = binarise(train_images)\n",
        "test_images = binarise(test_images)\n",
        "\n",
        "#print(f'train_images shape:{train_images.shape}')\n",
        "#print(f'test_images shape:{test_images.shape}')\n",
        "\n",
        "#print(f'train_labels shape:{train_labels.shape}')\n",
        "#print(f'test_labels shape:{test_labels.shape}')\n",
        "\n",
        "init_gen_params = init_net_params(param_scale, gen_layer_sizes)\n",
        "init_rec_params = init_net_params(param_scale, rec_layer_sizes)\n",
        "combined_init_params = (init_gen_params, init_rec_params)\n",
        "\n",
        "num_batches = int(np.ceil(len(train_images) / batch_size))\n",
        "def batch_indices(iter):\n",
        "    idx = iter % num_batches\n",
        "    return slice(idx * batch_size, (idx + 1) * batch_size)\n",
        "\n",
        "# Define training objective\n",
        "seed = npr.RandomState(0)\n",
        "def objective(combined_params, iter):\n",
        "    data_idx = batch_indices(iter)\n",
        "    gen_params, rec_params = combined_params\n",
        "    return -vae_lower_bound(gen_params, rec_params, train_images[data_idx],\n",
        "                            train_labels[data_idx], seed) / data_dim\n",
        "\n",
        "# Get gradients of objective using autograd.\n",
        "objective_grad = grad(objective)\n",
        "\n",
        "print(\"     Epoch     |    Objective       |    Test ELBO  \")\n",
        "def print_perf(combined_params, iter, grad):\n",
        "    if iter % 10 == 0:\n",
        "        gen_params, rec_params = combined_params\n",
        "        bound = np.mean(objective(combined_params, iter))\n",
        "        message = \"{:15}|{:20}|\".format(iter//num_batches, bound)\n",
        "        if iter % 100 == 0:\n",
        "            test_bound = -vae_lower_bound(gen_params, rec_params, test_images,\n",
        "                                          test_labels, seed) / data_dim\n",
        "            message += \"{:20}\".format(test_bound)\n",
        "        print(message)\n",
        "\n",
        "        fake_data = generate_from_prior(gen_params, 20, latent_dim, test_labels[:20],seed)\n",
        "        save_images(fake_data, 'vae_samples.png', vmin=0, vmax=1)\n",
        "\n",
        "# The optimizers provided can optimize lists, tuples, or dicts of parameters.\n",
        "optimized_params = adam(objective_grad, combined_init_params, step_size=step_size,\n",
        "                        num_iters=num_epochs * num_batches, callback=print_perf)\n",
        "\n",
        "np.argmax(test_labels[:20], axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading training data...\n",
            "Binarising training data...\n",
            "     Epoch     |    Objective       |    Test ELBO  \n",
            "              0|  0.6931072510892908|   0.693115652839441\n",
            "              0|  0.6841927304550305|\n",
            "              0|  0.6463121278821488|\n",
            "              0|  0.5335755386414979|\n",
            "              0| 0.40329663028977436|\n",
            "              0|  0.3134582342048598|\n",
            "              0|  0.2791614755544154|\n",
            "              0|  0.2938607796027756|\n",
            "              0|  0.2594956284146887|\n",
            "              0| 0.24165886904691442|\n",
            "              0| 0.26709156981207677|  0.2489107648303926\n",
            "              0| 0.26893226269126247|\n",
            "              0| 0.22794042495578723|\n",
            "              0| 0.23276435929256725|\n",
            "              0| 0.23591749115028962|\n",
            "              0|  0.2538492745088375|\n",
            "              0| 0.23973593272618118|\n",
            "              0|  0.2371972232456379|\n",
            "              0| 0.23945464261169733|\n",
            "              0| 0.23088238458090768|\n",
            "              0|  0.2277669700223499| 0.23004269612949443\n",
            "              0| 0.22636297992421356|\n",
            "              0| 0.23668770462883856|\n",
            "              0|  0.2199945661112304|\n",
            "              0| 0.23012905627507427|\n",
            "              0| 0.21424135850959458|\n",
            "              0| 0.23116231972396217|\n",
            "              0| 0.22669473727753825|\n",
            "              0|  0.2208304081247037|\n",
            "              0| 0.21715218893430557|\n",
            "              1| 0.21547231991323956| 0.22249228381212438\n",
            "              1|   0.216356830918994|\n",
            "              1|  0.2173206262647938|\n",
            "              1| 0.23002140565276705|\n",
            "              1|  0.2163833534011552|\n",
            "              1| 0.20939123547807048|\n",
            "              1| 0.22152608717592973|\n",
            "              1|  0.2434000302154285|\n",
            "              1| 0.22135468697185925|\n",
            "              1| 0.20568565728408858|\n",
            "              1|  0.2301356073870677| 0.21546470553741803\n",
            "              1| 0.24159616616294935|\n",
            "              1|  0.2029254168368736|\n",
            "              1| 0.20273038833127616|\n",
            "              1| 0.20480714913581993|\n",
            "              1| 0.22960919925092393|\n",
            "              1|  0.2169801848126462|\n",
            "              1|  0.2105001233171192|\n",
            "              1|  0.2162764842863167|\n",
            "              1| 0.20563001405671646|\n",
            "              1| 0.20318586423315704| 0.20671685203860143\n",
            "              1|  0.2018601449939252|\n",
            "              1| 0.21480417547859407|\n",
            "              1|  0.2017190615078938|\n",
            "              1| 0.20642588297366146|\n",
            "              1| 0.19466329888319378|\n",
            "              1|  0.2105721209054629|\n",
            "              1| 0.20611976346459202|\n",
            "              1|  0.1973752558232539|\n",
            "              1| 0.19606667091695196|\n",
            "              2| 0.19295002488841745| 0.19934728029070814\n",
            "              2| 0.19226005827607145|\n",
            "              2|  0.1927102466656732|\n",
            "              2| 0.20408943952587225|\n",
            "              2|  0.1936734425853064|\n",
            "              2| 0.18696460895149894|\n",
            "              2| 0.19212934163781448|\n",
            "              2| 0.21901407628873706|\n",
            "              2| 0.19676225047041102|\n",
            "              2| 0.18175817735257013|\n",
            "              2| 0.20972648134865646| 0.19234866355514657\n",
            "              2| 0.22433013866870805|\n",
            "              2|  0.1828520640065795|\n",
            "              2| 0.18251151459154424|\n",
            "              2| 0.18376242295440395|\n",
            "              2| 0.20619774035435015|\n",
            "              2| 0.19273140636195518|\n",
            "              2|  0.1880553550085294|\n",
            "              2| 0.19635947420624797|\n",
            "              2| 0.18447011119027484|\n",
            "              2| 0.18133167827142652| 0.18539308397102958\n",
            "              2| 0.17889282821161123|\n",
            "              2|  0.1960362366130649|\n",
            "              2|  0.1866326837857354|\n",
            "              2| 0.18582252365176855|\n",
            "              2|  0.1765716568514654|\n",
            "              2| 0.19019279807163417|\n",
            "              2|  0.1862585665119846|\n",
            "              2| 0.17912104361054004|\n",
            "              2| 0.17919747551249038|\n",
            "              3|  0.1740887323164608|    0.17963526159962\n",
            "              3| 0.17567498460109288|\n",
            "              3| 0.17486417406928656|\n",
            "              3| 0.18342045091338785|\n",
            "              3| 0.17530785270396204|\n",
            "              3| 0.16919067267165006|\n",
            "              3| 0.17032671299986654|\n",
            "              3| 0.20040420926448563|\n",
            "              3| 0.17773119659346576|\n",
            "              3| 0.16384922622861528|\n",
            "              3| 0.19133465626106175| 0.17317063950569675\n",
            "              3| 0.20909334141408756|\n",
            "              3| 0.16649935391756232|\n",
            "              3|  0.1662741969370238|\n",
            "              3| 0.16479844544009264|\n",
            "              3| 0.18583566110934652|\n",
            "              3| 0.17601313312799968|\n",
            "              3| 0.16968998301661425|\n",
            "              3| 0.17695533508180164|\n",
            "              3| 0.16852136789121874|\n",
            "              3| 0.16673503695873615| 0.16789171137643927\n",
            "              3| 0.16151528815839106|\n",
            "              3|  0.1794313557103937|\n",
            "              3| 0.17285163699154538|\n",
            "              3| 0.16823876440624858|\n",
            "              3| 0.16298596757406072|\n",
            "              3| 0.17597205442568686|\n",
            "              3| 0.17051080706724675|\n",
            "              3|  0.1654607489924078|\n",
            "              3| 0.16635966574665373|\n",
            "              4| 0.16108416165835296| 0.16496071609190846\n",
            "              4|  0.1630616401900139|\n",
            "              4|   0.159905367040427|\n",
            "              4| 0.16774203706717203|\n",
            "              4| 0.16300291849011658|\n",
            "              4| 0.15659836858598958|\n",
            "              4| 0.15695297091064833|\n",
            "              4|  0.1881721734048797|\n",
            "              4| 0.16490538416267214|\n",
            "              4| 0.15325517493814486|\n",
            "              4| 0.17959885144182539| 0.16092938275516686\n",
            "              4| 0.20113472274358216|\n",
            "              4| 0.15689145463158752|\n",
            "              4| 0.15503832291873398|\n",
            "              4| 0.15337810970351665|\n",
            "              4| 0.17455135657484608|\n",
            "              4| 0.16531917224312093|\n",
            "              4|  0.1607139140073993|\n",
            "              4| 0.16626423125844786|\n",
            "              4|  0.1583786997929031|\n",
            "              4|  0.1585328846255987| 0.15782710329697608\n",
            "              4|  0.1526879859752343|\n",
            "              4| 0.17025279656874207|\n",
            "              4| 0.16406779842103114|\n",
            "              4| 0.15906259958399718|\n",
            "              4|  0.1552414567845509|\n",
            "              4| 0.16810080821312923|\n",
            "              4| 0.16225765091251296|\n",
            "              4| 0.15682171611221765|\n",
            "              4|  0.1593413230532667|\n",
            "              5| 0.15380329703714476|  0.1566944179661411\n",
            "              5| 0.15445386953011603|\n",
            "              5| 0.15208438856364867|\n",
            "              5| 0.15872265243727743|\n",
            "              5| 0.15394861171688068|\n",
            "              5| 0.14944962871778336|\n",
            "              5| 0.14956912519949217|\n",
            "              5| 0.17991289978047265|\n",
            "              5|  0.1574731252888518|\n",
            "              5| 0.14675591680930344|\n",
            "              5| 0.17279930525185624|  0.1536731033901676\n",
            "              5| 0.19508750694096189|\n",
            "              5| 0.15143830817492865|\n",
            "              5| 0.14842960322793583|\n",
            "              5| 0.14661832917574064|\n",
            "              5| 0.16864963780786593|\n",
            "              5| 0.15944230201662984|\n",
            "              5| 0.15408535907017737|\n",
            "              5|   0.158830167365225|\n",
            "              5| 0.15172741704565437|\n",
            "              5| 0.15235352741840513| 0.15139540634247795\n",
            "              5| 0.14680998683864072|\n",
            "              5| 0.16371515643334705|\n",
            "              5|  0.1583564641345515|\n",
            "              5| 0.15377179964121063|\n",
            "              5| 0.15057138156269867|\n",
            "              5| 0.16353936567870045|\n",
            "              5| 0.15653255772961427|\n",
            "              5| 0.15096548695682582|\n",
            "              5| 0.15242750952648002|\n",
            "              6| 0.14798933887706733| 0.15109370058075616\n",
            "              6| 0.14853575029078483|\n",
            "              6| 0.14647892405719043|\n",
            "              6| 0.15248948433400863|\n",
            "              6|  0.1481789809446243|\n",
            "              6| 0.14478981025771678|\n",
            "              6|   0.144532912852847|\n",
            "              6|  0.1739026522723776|\n",
            "              6| 0.15248326203325238|\n",
            "              6| 0.14226603261569887|\n",
            "              6| 0.16703326840491692|   0.148223414313794\n",
            "              6| 0.19071393310741788|\n",
            "              6| 0.14658806155607332|\n",
            "              6| 0.14277458287525913|\n",
            "              6|  0.1413679517792218|\n",
            "              6| 0.16335387137460003|\n",
            "              6|  0.1547019731061795|\n",
            "              6| 0.14920422276250767|\n",
            "              6| 0.15279115124537646|\n",
            "              6| 0.14751527856282143|\n",
            "              6|  0.1470836167371612|  0.1462562926222373\n",
            "              6| 0.14203864373102576|\n",
            "              6| 0.15977723570811633|\n",
            "              6| 0.15327126534230892|\n",
            "              6|  0.1493025756391368|\n",
            "              6| 0.14635169013451219|\n",
            "              6| 0.15872976082749166|\n",
            "              6|  0.1524099159424184|\n",
            "              6| 0.14689517561182192|\n",
            "              6| 0.14827709964757044|\n",
            "              7| 0.14361148530514914| 0.14603837742693454\n",
            "              7|  0.1429064955582579|\n",
            "              7| 0.14218573151668784|\n",
            "              7| 0.14804329134403624|\n",
            "              7|  0.1431904594013332|\n",
            "              7| 0.14131668891600196|\n",
            "              7| 0.13977063642575951|\n",
            "              7|  0.1676335956344008|\n",
            "              7| 0.14751919600525126|\n",
            "              7| 0.13844915542576428|\n",
            "              7|  0.1618552407200776| 0.14328738933627536\n",
            "              7|  0.1859223466206456|\n",
            "              7| 0.14333492638797085|\n",
            "              7|  0.1380682547807571|\n",
            "              7| 0.13747241826027967|\n",
            "              7|  0.1587532651175505|\n",
            "              7| 0.14907891520328603|\n",
            "              7| 0.14468720765139695|\n",
            "              7| 0.14813081435913275|\n",
            "              7| 0.14337533415610637|\n",
            "              7|  0.1425060369919737| 0.14154157917319454\n",
            "              7| 0.13753235747392928|\n",
            "              7| 0.15532777840525583|\n",
            "              7|  0.1486138728682386|\n",
            "              7| 0.14472163409678632|\n",
            "              7|  0.1422586619947452|\n",
            "              7| 0.15471168479107744|\n",
            "              7| 0.14761317802140123|\n",
            "              7| 0.14211264993237338|\n",
            "              7|  0.1442459971345199|\n",
            "              8|  0.1394129929768734| 0.14184435935632234\n",
            "              8|   0.137764263817647|\n",
            "              8| 0.13783255518681123|\n",
            "              8| 0.14384440049961866|\n",
            "              8| 0.13905858965315154|\n",
            "              8|  0.1368976440802394|\n",
            "              8|   0.135220304930977|\n",
            "              8| 0.16154740421646804|\n",
            "              8| 0.14290969880689494|\n",
            "              8| 0.13488380697748395|\n",
            "              8|  0.1566759781461162| 0.13905401599544714\n",
            "              8| 0.18239263978958153|\n",
            "              8| 0.13914912854222036|\n",
            "              8| 0.13448363219270315|\n",
            "              8| 0.13368264355973974|\n",
            "              8| 0.15484249955141244|\n",
            "              8| 0.14547062375435835|\n",
            "              8| 0.14116337284215527|\n",
            "              8| 0.14389026143163156|\n",
            "              8| 0.13958827677465555|\n",
            "              8|  0.1387365667874142| 0.13763925487110656\n",
            "              8|  0.1334537474012626|\n",
            "              8| 0.15168678237648298|\n",
            "              8| 0.14443487328688634|\n",
            "              8|  0.1414703618727412|\n",
            "              8| 0.13823660672383148|\n",
            "              8| 0.15069052191453558|\n",
            "              8| 0.14418713963016386|\n",
            "              8| 0.13888053912242648|\n",
            "              8| 0.14143548580985363|\n",
            "              9|  0.1369743152739578| 0.13882220777333862\n",
            "              9| 0.13394535058541165|\n",
            "              9|  0.1345126809757784|\n",
            "              9|  0.1409805195620413|\n",
            "              9| 0.13619057343196656|\n",
            "              9|  0.1340752606192431|\n",
            "              9| 0.13138757453874683|\n",
            "              9| 0.15719879031494086|\n",
            "              9|  0.1393340044809218|\n",
            "              9| 0.13174170461542678|\n",
            "              9| 0.15279871252658064|  0.1358182151038678\n",
            "              9| 0.17935100247507849|\n",
            "              9| 0.13669847130956306|\n",
            "              9| 0.13096221541770522|\n",
            "              9|  0.1311892034983362|\n",
            "              9| 0.15146942412606518|\n",
            "              9| 0.14256158514884207|\n",
            "              9| 0.13789606982507194|\n",
            "              9|  0.1399714495071456|\n",
            "              9|  0.1358679859582905|\n",
            "              9| 0.13581014802217906| 0.13473406522431608\n",
            "              9| 0.13035025354981908|\n",
            "              9| 0.14871780535727072|\n",
            "              9| 0.14145263572745478|\n",
            "              9| 0.13927997546014756|\n",
            "              9| 0.13517653160498908|\n",
            "              9| 0.14853311552682946|\n",
            "              9| 0.14150868594610794|\n",
            "              9| 0.13660249985731604|\n",
            "              9| 0.13878825932180744|\n",
            "             10| 0.13417107986761242| 0.13587070214559938\n",
            "             10|  0.1309686261174226|\n",
            "             10| 0.13235986849305678|\n",
            "             10| 0.13830153460930117|\n",
            "             10| 0.13312197467745623|\n",
            "             10| 0.13205301719376167|\n",
            "             10|   0.128415155318039|\n",
            "             10| 0.15322953089220048|\n",
            "             10|  0.1370707680694268|\n",
            "             10| 0.12941339934378973|\n",
            "             10| 0.15014374585645415| 0.13315057544635603\n",
            "             10|  0.1758891654269865|\n",
            "             10|   0.134218594416782|\n",
            "             10| 0.12924331567862077|\n",
            "             10| 0.12842097746845055|\n",
            "             10| 0.14935768387193704|\n",
            "             10| 0.13891492229205513|\n",
            "             10| 0.13508682554759244|\n",
            "             10| 0.13819569976276597|\n",
            "             10| 0.13341813152784035|\n",
            "             10| 0.13315629791430175| 0.13221900507663137\n",
            "             10| 0.12767360442655296|\n",
            "             10| 0.14652931032625682|\n",
            "             10|  0.1390110019485974|\n",
            "             10|  0.1370478346194762|\n",
            "             10| 0.13357131327317404|\n",
            "             10| 0.14584639894028104|\n",
            "             10| 0.13909601747017372|\n",
            "             10| 0.13392526017261608|\n",
            "             10| 0.13665323626383233|\n",
            "             11| 0.13231057395549545| 0.13391391243709871\n",
            "             11|  0.1285170045340635|\n",
            "             11| 0.13064846212263495|\n",
            "             11|  0.1361160220858845|\n",
            "             11| 0.13089736090830764|\n",
            "             11|  0.1306682723504093|\n",
            "             11| 0.12607341313234285|\n",
            "             11| 0.14967288586835523|\n",
            "             11| 0.13472890013256392|\n",
            "             11|  0.1269834698880481|\n",
            "             11|  0.1476752296156836|  0.1311421792299094\n",
            "             11|   0.172753972850096|\n",
            "             11|  0.1328707434983298|\n",
            "             11| 0.12666605907845888|\n",
            "             11| 0.12607090406874405|\n",
            "             11| 0.14622769483786494|\n",
            "             11| 0.13704474449571452|\n",
            "             11| 0.13305517795744135|\n",
            "             11|  0.1358145724653095|\n",
            "             11|  0.1319602511455402|\n",
            "             11| 0.13133164992515273| 0.13019284709374793\n",
            "             11| 0.12627362767408062|\n",
            "             11| 0.14387582077866207|\n",
            "             11| 0.13737976146341474|\n",
            "             11|  0.1353647563526277|\n",
            "             11| 0.13201938117919174|\n",
            "             11| 0.14337041326622957|\n",
            "             11| 0.13679385801181465|\n",
            "             11|  0.1322638060014094|\n",
            "             11| 0.13483640280830714|\n",
            "             12| 0.13041917686019094| 0.13200093068905117\n",
            "             12|  0.1265372929106256|\n",
            "             12|  0.1282417946730073|\n",
            "             12| 0.13403679997251694|\n",
            "             12|  0.1284872662295836|\n",
            "             12|  0.1284551864875381|\n",
            "             12| 0.12425182175752648|\n",
            "             12| 0.14731648404527936|\n",
            "             12|  0.1332103167318998|\n",
            "             12| 0.12503158650229973|\n",
            "             12| 0.14608766200573925| 0.12928705330504608\n",
            "             12| 0.17079183091865846|\n",
            "             12| 0.13110863915556228|\n",
            "             12| 0.12561238523396584|\n",
            "             12|  0.1249615595691825|\n",
            "             12| 0.14415146479526972|\n",
            "             12| 0.13565598939610593|\n",
            "             12| 0.13071331217757443|\n",
            "             12|   0.133642642656948|\n",
            "             12| 0.13008674149742272|\n",
            "             12|  0.1298383525698772| 0.12870799537177105\n",
            "             12| 0.12504954996425055|\n",
            "             12| 0.14217700234580782|\n",
            "             12| 0.13532978281505798|\n",
            "             12| 0.13383851109994135|\n",
            "             12| 0.13021698515085597|\n",
            "             12| 0.14150201357282574|\n",
            "             12| 0.13554636330297207|\n",
            "             12| 0.13060629501444795|\n",
            "             12| 0.13350992963458244|\n",
            "             13| 0.12887888334683312| 0.13074912300396713\n",
            "             13| 0.12537941173357514|\n",
            "             13|   0.126126051204463|\n",
            "             13| 0.13269020179292068|\n",
            "             13| 0.12759200207376456|\n",
            "             13| 0.12653077728760462|\n",
            "             13| 0.12276733889783122|\n",
            "             13| 0.14437064136081956|\n",
            "             13| 0.13191240324813472|\n",
            "             13| 0.12379974920635009|\n",
            "             13| 0.14419905332382146| 0.12774748025964855\n",
            "             13| 0.16928660147469965|\n",
            "             13| 0.12961654047680204|\n",
            "             13|  0.1239055949778716|\n",
            "             13| 0.12347473571443382|\n",
            "             13| 0.14205846689246912|\n",
            "             13|  0.1346343903155329|\n",
            "             13| 0.12906059686512023|\n",
            "             13|  0.1325011362455937|\n",
            "             13|  0.1290999998182884|\n",
            "             13| 0.12818963003451558|  0.1272043355971996\n",
            "             13| 0.12341289090214945|\n",
            "             13| 0.14143818072388378|\n",
            "             13|  0.1339309990322427|\n",
            "             13| 0.13233589398687376|\n",
            "             13| 0.12892855870051514|\n",
            "             13|  0.1402831608416618|\n",
            "             13|  0.1343460033734953|\n",
            "             13| 0.12930456228362386|\n",
            "             13| 0.13192480475533613|\n",
            "             14| 0.12756917661836598| 0.12985725845837892\n",
            "             14| 0.12329214978939235|\n",
            "             14|  0.1259750391983014|\n",
            "             14| 0.13185917754352974|\n",
            "             14| 0.12564361980207836|\n",
            "             14| 0.12640993392799643|\n",
            "             14| 0.12139565627042195|\n",
            "             14| 0.14347258638292704|\n",
            "             14| 0.13106676017266194|\n",
            "             14| 0.12198654789514632|\n",
            "             14| 0.14204758297913495| 0.12659580110267266\n",
            "             14| 0.16781610337040181|\n",
            "             14|  0.1280489301943507|\n",
            "             14| 0.12235571777060919|\n",
            "             14| 0.12218086386292645|\n",
            "             14| 0.14015155333616236|\n",
            "             14| 0.13280509816342764|\n",
            "             14| 0.12784701884298022|\n",
            "             14|  0.1312519924294651|\n",
            "             14| 0.12784272598264584|\n",
            "             14|  0.1265506597896858| 0.12575415570742304\n",
            "             14| 0.12213061062861438|\n",
            "             14| 0.13983419107552214|\n",
            "             14| 0.13258853798839446|\n",
            "             14| 0.13097139857567977|\n",
            "             14| 0.12757424616666985|\n",
            "             14| 0.13882184361258257|\n",
            "             14| 0.13279984112984322|\n",
            "             14|  0.1281962143340946|\n",
            "             14|  0.1305966573483296|\n",
            "             15| 0.12696293462041283| 0.12892592228396282\n",
            "             15| 0.12168093120084594|\n",
            "             15|  0.1241836110667151|\n",
            "             15| 0.12995738006505914|\n",
            "             15| 0.12461948165557236|\n",
            "             15|  0.1241040909403125|\n",
            "             15|   0.120205974769527|\n",
            "             15| 0.14056856384660574|\n",
            "             15|  0.1285879463528105|\n",
            "             15| 0.12064355754394729|\n",
            "             15| 0.14027746749484388| 0.12521714690591054\n",
            "             15| 0.16634221190349974|\n",
            "             15| 0.12691648711684797|\n",
            "             15| 0.12085002876059588|\n",
            "             15|  0.1207690591597668|\n",
            "             15| 0.13894588879185207|\n",
            "             15|  0.1316329932675981|\n",
            "             15|  0.1264149113163881|\n",
            "             15| 0.12952281541229285|\n",
            "             15| 0.12707912460454718|\n",
            "             15| 0.12582731011598186|  0.1246344867919331\n",
            "             15| 0.12031299249046289|\n",
            "             15| 0.13889936218980187|\n",
            "             15| 0.13097628631401412|\n",
            "             15| 0.13033257541482035|\n",
            "             15| 0.12636018600605475|\n",
            "             15| 0.13716505018641267|\n",
            "             15| 0.13176968973786696|\n",
            "             15|  0.1260151082640559|\n",
            "             15| 0.12923744999688044|\n",
            "             16|  0.1256411459498501| 0.12738680283409395\n",
            "             16| 0.12036004544319222|\n",
            "             16| 0.12335803614985648|\n",
            "             16| 0.12936614358762208|\n",
            "             16| 0.12286193439005644|\n",
            "             16|   0.123249170616394|\n",
            "             16| 0.11842604767554057|\n",
            "             16|  0.1387782816651319|\n",
            "             16|  0.1277050963240891|\n",
            "             16| 0.11934474689454674|\n",
            "             16| 0.13892557153620214| 0.12381478902320915\n",
            "             16| 0.16452569116531446|\n",
            "             16| 0.12490700781063531|\n",
            "             16| 0.12028653119370532|\n",
            "             16| 0.11998146689188129|\n",
            "             16| 0.13624403103813937|\n",
            "             16| 0.12939841426114093|\n",
            "             16| 0.12511915721536543|\n",
            "             16| 0.12850316908089163|\n",
            "             16| 0.12478617154591236|\n",
            "             16| 0.12452072577271173| 0.12335427766854964\n",
            "             16| 0.11949632959331193|\n",
            "             16| 0.13666754455975894|\n",
            "             16| 0.13044281712232614|\n",
            "             16| 0.12930900784093913|\n",
            "             16|  0.1248561892563906|\n",
            "             16| 0.13568039466318318|\n",
            "             16| 0.13037399605878694|\n",
            "             16| 0.12473365067451461|\n",
            "             16| 0.12809168970563542|\n",
            "             17| 0.12420982698566072| 0.12652191613961716\n",
            "             17| 0.11937117676118433|\n",
            "             17| 0.12194278944703794|\n",
            "             17| 0.12758168938718653|\n",
            "             17| 0.12081863240098524|\n",
            "             17| 0.12257698862146085|\n",
            "             17| 0.11733037230299546|\n",
            "             17| 0.13635992154022214|\n",
            "             17| 0.12545304826412662|\n",
            "             17|  0.1179908326905423|\n",
            "             17| 0.13740566558023562|  0.1225904213482159\n",
            "             17|  0.1635564433630003|\n",
            "             17| 0.12375992967042126|\n",
            "             17| 0.11901433125286516|\n",
            "             17| 0.11887365208181494|\n",
            "             17| 0.13494593410388517|\n",
            "             17|  0.1284493076874176|\n",
            "             17| 0.12389643848818765|\n",
            "             17| 0.12675067649402838|\n",
            "             17| 0.12499387685612522|\n",
            "             17|  0.1232377239528077| 0.12201843253484206\n",
            "             17| 0.11799498472232746|\n",
            "             17|  0.1357265321747989|\n",
            "             17|  0.1284269307298534|\n",
            "             17| 0.12812804527499713|\n",
            "             17|  0.1237883788346084|\n",
            "             17|  0.1346556850811532|\n",
            "             17| 0.12920997346772398|\n",
            "             17| 0.12315518596338773|\n",
            "             17| 0.12720245409606673|\n",
            "             18| 0.12289386157690407| 0.12514004326835346\n",
            "             18| 0.11886466184734643|\n",
            "             18| 0.12062053610571516|\n",
            "             18| 0.12596609207659218|\n",
            "             18| 0.12067454634054986|\n",
            "             18| 0.12073928837458184|\n",
            "             18| 0.11626485805633315|\n",
            "             18|   0.135000920717142|\n",
            "             18| 0.12457230892661493|\n",
            "             18|  0.1168015139787481|\n",
            "             18| 0.13495300331240825|  0.1212375473457321\n",
            "             18|  0.1604379934780377|\n",
            "             18| 0.12174439576556914|\n",
            "             18| 0.11782169040368515|\n",
            "             18| 0.11774952379351199|\n",
            "             18| 0.13469979799069126|\n",
            "             18|  0.1270391733957348|\n",
            "             18| 0.12190617172266849|\n",
            "             18| 0.12596405013890744|\n",
            "             18| 0.12334294767438624|\n",
            "             18| 0.12229868853125764| 0.12075871964716835\n",
            "             18| 0.11650713188120178|\n",
            "             18| 0.13479694097527006|\n",
            "             18| 0.12775783376774472|\n",
            "             18| 0.12662684318718764|\n",
            "             18| 0.12328218448639347|\n",
            "             18| 0.13294350358826507|\n",
            "             18|  0.1270291821709256|\n",
            "             18| 0.12195098460280814|\n",
            "             18| 0.12603881042618037|\n",
            "             19| 0.12078699707611962| 0.12377811061781802\n",
            "             19| 0.11676157672370402|\n",
            "             19| 0.11962209194651907|\n",
            "             19| 0.12492291414476511|\n",
            "             19| 0.11876427664359368|\n",
            "             19| 0.11999407633762901|\n",
            "             19|  0.1147311353798246|\n",
            "             19| 0.13263873680767735|\n",
            "             19| 0.12364885672763645|\n",
            "             19| 0.11536322814575202|\n",
            "             19| 0.13400443127385597| 0.11997480740973306\n",
            "             19| 0.15941001236013874|\n",
            "             19| 0.12070171692656483|\n",
            "             19| 0.11706311091512162|\n",
            "             19| 0.11628648899460349|\n",
            "             19| 0.13243112747818922|\n",
            "             19| 0.12602621946913475|\n",
            "             19| 0.12054988700698162|\n",
            "             19| 0.12497386224529937|\n",
            "             19| 0.12197405737779611|\n",
            "             19| 0.12038556338628216| 0.11962520054110148\n",
            "             19|  0.1150110898755059|\n",
            "             19| 0.13314541412343603|\n",
            "             19|  0.1267052786584494|\n",
            "             19|  0.1258282846003487|\n",
            "             19| 0.12255650331194834|\n",
            "             19| 0.13163814162140636|\n",
            "             19|  0.1259049896862982|\n",
            "             19| 0.12064799099569888|\n",
            "             19| 0.12435802415371261|\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymHUTsMXxq45",
        "outputId": "5d7876dc-e5b7-4e6c-d951-5b94adcc1672",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.argmax(test_labels[:20], axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHuwYvFx_WTR"
      },
      "source": [
        "Implement VAEAC\n",
        "\n",
        "Reference - https://github.com/tigvarts/vaeac\n",
        "\n",
        "https://github.com/asahi417/ConditionalVariationalAutoEncoder/blob/master/model/cvae_cnn2.py\n",
        "\n",
        "https://wiseodd.github.io/techblog/2016/12/17/conditional-vae/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VQUXrvaLLsF"
      },
      "source": [
        "# VAEAC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LK2deqUAFBZ"
      },
      "source": [
        "# source: https://github.com/azraelzhor/tf2-VAEAC\n",
        "class MixtureMaskGenerator:\n",
        "\n",
        "  def __init__(self, generators, weights):\n",
        "    self.generators = generators\n",
        "    self.weights = weights\n",
        "\n",
        "  def __call__(self, inputs):\n",
        "    w = np.array(self.weights, dtype='float')\n",
        "    w /= w.sum()\n",
        "    c_ids = np.random.choice(w.size, inputs.shape[0], True, w)\n",
        "    mask = np.zeros_like(inputs)\n",
        "\n",
        "    for i, gen in enumerate(self.generators):\n",
        "      ids = np.where(c_ids == i)[0]\n",
        "      if len(ids) == 0:\n",
        "        continue\n",
        "      samples = gen(tf.gather(inputs, ids, axis=0))\n",
        "      mask[ids] = samples\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9MzF2niReM6"
      },
      "source": [
        "VAEAC TF2 Implementation Reference: https://github.com/azraelzhor/tf2-VAEAC\n",
        "\n",
        "fc_gauss.py reference - https://drive.google.com/drive/folders/1nxU0G3blygdlwuMJ6nddVq207hWzn71l\n",
        "\n",
        "VAEAC Paper - https://arxiv.org/pdf/1806.02382.pdf\n",
        "\n",
        "CLUE Paper - https://arxiv.org/pdf/2006.06848.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2M9nr5_DI_r"
      },
      "source": [
        "training - https://colab.research.google.com/drive/1UGkyJR49QzAjsSUZzOsoqdSoxKR9PwL8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHALjEohdnEA"
      },
      "source": [
        "VAEAC models conditional distribution over targets given inputs, $p_{gt}(y|x)$. why do we need arbitrary conditioning when we have specific targets $y$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW9cW6u6bcJ7"
      },
      "source": [
        "Role:\n",
        "\n",
        "- Produce ground truth data to train VAE and BNN\n",
        "- Provide input to VAE and BNN during counterfactual generation\n",
        "- Use  $p_{gt}(y|x_c)$ in both aleotoric and epistemic uncertainty calculations\n",
        "- Assess the relevance of counterfactuals through their likelihood under the g.t. VAEAC $log(p_{gt}(x_c))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrQqaSUCABqy"
      },
      "source": [
        "# \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo-ZTm8FLIfP"
      },
      "source": [
        "\n",
        "# from models import prior_network, proposal_network, generative_network\n",
        "\n",
        "class VAEAC:\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        # networks\n",
        "        self.prior_net = prior_network\n",
        "        self.proposal_net = proposal_network\n",
        "        self.generative_net = generative_network\n",
        "        self.config = config #optimizer, epochs etc\n",
        "\n",
        "\n",
        "    # mask to generate arbitrary conditions\n",
        "    @staticmethod\n",
        "    def generate_mask(x):\n",
        "        return \n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_mask(x, mask):\n",
        "      # input with mask applied\n",
        "      return\n",
        "\n",
        "\n",
        "\n",
        "    def prior_distribution(self, x, mask):\n",
        "      # apply masks to input\n",
        "\n",
        "      # pass input through prior network to generate mu and sigma\n",
        "\n",
        "      # produce normal distribution with mu and sigma\n",
        "      return\n",
        "\n",
        "\n",
        "\n",
        "    def prior_regularize(self, prior):\n",
        "        '''\n",
        "        the parameters of the prior distribution of z may tend\n",
        "        to infinity, since there is no penalty for large values of those parameters\n",
        "        '''\n",
        "        # return mu_regularizer and sigma_regularizer to be added to the model log-likelihood\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    def proposal_distribution(self, x):\n",
        "      # need masks as input??\n",
        "\n",
        "      # pass input through proposal (posterior) network to generate mu and sigma\n",
        "\n",
        "      # produce normal distributions with mus and sigmas\n",
        "      return\n",
        "\n",
        "\n",
        "\n",
        "    def generative_network_params(self, z_sample):\n",
        "        g_params = self.generative_network(z_sample)\n",
        "        return g_params\n",
        "\n",
        "\n",
        "\n",
        "    def generatve_distribution(self, z_sample):\n",
        "        g_params = generative_network_params(self, z_sample)\n",
        "        return g_params\n",
        "\n",
        "\n",
        "\n",
        "    def compute_loss(self, inputs, masks):\n",
        "\n",
        "      # Prior distribution ---------------------------------\n",
        "\n",
        "      # get prior distribution (Gaussian)\n",
        "      prior_dist = prior_distribution(self, x, mask)\n",
        "\n",
        "      # prior regularization to be added to the model log-likelihood\n",
        "      prior_regularization = None # prior_regularization = self.prior_regularizer(prior_dist)\n",
        "\n",
        "\n",
        "      # Proposal distribution ------------------------------\n",
        "\n",
        "      # get proposal distribution (Gaussian) TODO - requires mask inputs?\n",
        "      proposal_dist = proposal_distribution(self, x)\n",
        "\n",
        "      # sample latents from proposal_dist\n",
        "      latent = None\n",
        "\n",
        "\n",
        "      # Generative distribution ----------------------------\n",
        "\n",
        "      generative_params = self.generative_net(latent)\n",
        "\n",
        "\n",
        "\n",
        "      # calculate loglikelihood of output of generative distribution out\n",
        "      loglike = None\n",
        "      \n",
        "      # calculate KL divergence\n",
        "      kl = None # kl_divergence(proposal_distribution, prior_distribution)\n",
        "        \n",
        "      return loglike - kl + prior_regularization\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "      def fit(self):\n",
        "\n",
        "\n",
        "        for epoch in range(1, self.config.epochs + 1):\n",
        "\n",
        "          # generate masks for data set\n",
        "\n",
        "          # compute gradiates of loss function\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    def elbo(self, prior, approx_post, x, rec_params):\n",
        "        # calculate log likelihood\n",
        "\n",
        "        # calculate KL divergence\n",
        "        # kl = kl_divergence(approx_post, prior)\n",
        "        \n",
        "        # calculate prior_regularization\n",
        "\n",
        "        return # loglike - kl + prior_regularization\n",
        "\n",
        "    def iwlb(self, prior, approx_post, x, n=100):\n",
        "      # Importance Weighted Lower Bound \n",
        "      estimates = []\n",
        "        for i in range(n):\n",
        "\n",
        "          # TODO reparameterize\n",
        "          # sample from latent space\n",
        "\n",
        "          # calculate sum of log_prior of latents\n",
        "\n",
        "          # calculate sum of log_posterior of latents\n",
        "\n",
        "          # generate mu and sigma from generative network\n",
        "\n",
        "          # calculate log likelihood of the model\n",
        "\n",
        "          # est = rec_loglike + prior_log_prob - proposal_log_prob\n",
        "          # estimates.append(est)\n",
        "\n",
        "        return #logsumexp of estimates - np.log(n) ???\n",
        "\n",
        "          \n",
        "\n",
        "    \n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    def fit(self, x):\n",
        "\n",
        "        approx_post = self.proposal_distribution(x)\n",
        "\n",
        "        #sample from posterior\n",
        "        z_sample = None\n",
        "\n",
        "        rec_params = self.generator_params(z_sample)\n",
        "\n",
        "        # calculate loss on ELBO\n",
        "\n",
        "        return\n",
        "    '''\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "      \n",
        "#objective_grad = grad(objective)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWnyCH7WSpTN"
      },
      "source": [
        "Prior Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wON-NqQ7SsT_"
      },
      "source": [
        "Proposal (posterior Network)\n",
        "\n",
        "input: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kE-HqmDS0DS"
      },
      "source": [
        "Generator network"
      ]
    }
  ]
}